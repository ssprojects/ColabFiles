(17 June 2019) Final plots on the right way of incorporating precision recall constraints on all data.
(17 June 2019) Bayesian model for continuous attributes.
(17 June 2019) More datasets

(12 June 2019) Question classification: create multiple overlapping fine-grained LFs
 
(12 June 2019) Why does likelihood function become negative for continuous s?

(12 June 2019) Current code does not seem to be the version that worked for SS. Raghav will try to fix, else SS will have to look.

(12, 4 June 2019) Repeat expts with true precision and recall constraints as implemented in the latest checked in code.

(4 June 2019) Repeat expts with soft-plus instead of binomial loss for constraints.

(4 June 2019) Explore what potentials and constraints will work for continous LFs.  Test on datasets with several high-quality continuous LFs, e.g. dedup, classification datasets with keywords

(12, 4 June 2019) Add more datasets (Snorkel new papers include some, e.g.)

   https://ieeexplore.ieee.org/abstract/document/8609589/authors#authors

   https://www.paroma.xyz/tech_report_reef.pdf (IMDB dataset?)

    https://towardsdatascience.com/a-technique-for-building-nlp-classifiers-efficiently-with-transfer-learning-and-weak-supervision-a8e2f21ca9c8

5. (Old) Move to half-gaussian continuous attributes and make sure the parameters that are learned with the
matched potential form are again consistent with hand-calculated values.

4.  Write testcases in CodeTest.ipny for various probability and precision loss computations.


