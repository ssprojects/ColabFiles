{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Synthetic_Benchmark_smooth_1_1-Copy1.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "xa3_fAvgD_Ts",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#call this only once for a kernel startup\n",
        "from __future__ import absolute_import, division, print_function\n",
        "import model_only\n",
        "\n",
        "import tensorflow as tf\n",
        "# BATCH_SIZE = 32\n",
        "seed = 12\n",
        "\n",
        "import sys\n",
        "orig_stdout = sys.stdout"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uz4oeBfkFd4k",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "guW5EThLD_UG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "581ce668-4534-4beb-c6cd-11b4509a723e"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from numpy import genfromtxt\n",
        "from numpy import random as rand\n",
        "import math\n",
        "\n",
        "seed = 12\n",
        "rand.seed(12)\n",
        "\n",
        "instances = 1000\n",
        "epsilon = 0.02 # 0.50 + epsilon = correlation\n",
        "ratio = 0.5 # ones:total\n",
        "noOfFunctions = 50\n",
        "\n",
        "correlatedInstances = math.ceil((0.5+epsilon)*instances)\n",
        "indices = np.arange(0,instances,1)\n",
        "\n",
        "numOnes = math.ceil(ratio*instances)\n",
        "numZeros = instances - numOnes\n",
        "onesArray = np.ones(numOnes)#, dtype=int)\n",
        "zerosArray = np.zeros(numZeros)#, dtype=int)\n",
        "labels = np.concatenate((onesArray, zerosArray))\n",
        "rand.shuffle(labels) #shuffle datapoints\n",
        "\n",
        "print(np.count_nonzero(labels), correlatedInstances)\n",
        "\n",
        "functions=[]\n",
        "for i in range(math.ceil(noOfFunctions/2)):\n",
        "    index_i = rand.choice(indices, size = correlatedInstances, replace=False)\n",
        "    f = [1-labels, np.random.uniform(0,0.75, len(labels))]\n",
        "    for j in index_i:\n",
        "        f[1][j] = np.random.uniform(0.25, 1)\n",
        "        f[0][j] = labels[j]\n",
        "    functions.append(f)\n",
        "\n",
        "for i in range(math.floor(noOfFunctions/2)):\n",
        "    index_i = rand.choice(indices, size = correlatedInstances, replace=False)\n",
        "    f = [-1*labels, np.random.uniform(0, 0.75, len(labels))]\n",
        "    for j in index_i:\n",
        "        f[1][j] = np.random.uniform(0.25, 1)\n",
        "        f[1][j] = labels[j] - 1\n",
        "    functions.append(f)\n",
        "    \n",
        "functions.append([labels, labels])\n",
        "functions = np.transpose(np.array(functions))\n",
        "print(functions.shape)\n",
        "y=functions\n",
        "# print(y[0])\n",
        "print(y.shape)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "500 520\n",
            "(1000, 2, 51)\n",
            "(1000, 2, 51)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-bCw60ARD_UY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "cac82042-e0a6-47c0-af62-9c93e187fb8e"
      },
      "cell_type": "code",
      "source": [
        "last = len(y[0][0])-1\n",
        "ones = [p for p in y if p[0][last]==1]\n",
        "zeros = [p for p in y if p[0][last]==0]\n",
        "\n",
        "print(len(zeros),len(ones),len(y))\n",
        "\n",
        "train_set=[]\n",
        "test_set=[]\n",
        "\n",
        "split=0.8\n",
        "\n",
        "# split into train and test\n",
        "# same distribution of 1s\n",
        "\n",
        "train_split=int(split*len(y))\n",
        "test_split=len(y)-train_split\n",
        "\n",
        "train_ones = int(split*len(ones))\n",
        "test_ones = len(ones)-train_ones\n",
        "train_zeros = train_split-train_ones\n",
        "test_zeros = test_split-test_ones\n",
        "\n",
        "print(train_ones, test_ones, train_zeros, test_zeros)\n",
        "j=0\n",
        "k=0\n",
        "for i in range(train_split):\n",
        "    if(j<i*train_ones/train_split):\n",
        "        train_set.append(ones[j])\n",
        "        j = j +1\n",
        "    else:\n",
        "        train_set.append(zeros[k])\n",
        "        k = k +1\n",
        "train_set = np.array(train_set)\n",
        "print(train_set.shape)\n",
        "# print(np.sum(train_set[:,last]), len(train_set))\n",
        "\n",
        "j2=0\n",
        "k2=0\n",
        "for i in range(test_split):\n",
        "    if(j2<i*test_ones/test_split):\n",
        "        test_set.append(ones[j])\n",
        "        j = j +1\n",
        "        j2= j2+1\n",
        "    else:\n",
        "        test_set.append(zeros[k])\n",
        "        k = k +1\n",
        "        k2= k2+1\n",
        "test_set = np.array(test_set)\n",
        "print(test_set.shape)\n",
        "rand.shuffle(train_set)\n",
        "rand.shuffle(test_set)\n",
        "# print(np.sum(test_set[:,last]), len(test_set))\n",
        "  "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "500 500 1000\n",
            "400 100 400 100\n",
            "(800, 2, 51)\n",
            "(200, 2, 51)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "e0CSS5a2D_Uo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "np.save(\"train_synthetic_smooth\",train_set)\n",
        "np.save(\"test_synthetic_smooth\", test_set)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fkaMrK8pD_U9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "0032964f-d47e-4dc3-fc9f-c99e293ede9a"
      },
      "cell_type": "code",
      "source": [
        "from numpy import genfromtxt\n",
        "temp = np.load(\"test_synthetic_smooth.npy\")\n",
        "true_labels = temp[:,0,last]\n",
        "gold_labels_dev = true_labels\n",
        "gold_labels_test = true_labels\n",
        "print(true_labels.shape)\n",
        "print(np.count_nonzero(true_labels))\n",
        "print(true_labels[:20])\n",
        "\n",
        "# mapping class labels to labeling functions\n",
        "# can consider distance functions as giving a positively correlated score to the negative class\n",
        "LF_l = np.concatenate((np.ones(math.ceil(noOfFunctions/2)), -1+np.zeros(math.floor(noOfFunctions/2))))\n",
        "print(len(LF_l), LF_l)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(200,)\n",
            "100\n",
            "[1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1.]\n",
            "50 [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
            "  1.  1.  1.  1.  1.  1.  1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
            " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_qO_wDTmD_VN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from collections import defaultdict\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import average_precision_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def draw2DArray(a):\n",
        "    fig = plt.figure(figsize=(6, 3.2))\n",
        "    ax = fig.add_subplot(111)\n",
        "    ax.set_title('colorMap')\n",
        "    plt.imshow(np.array(a))\n",
        "    ax.set_aspect('equal')\n",
        "    cax = fig.add_axes([0.12, 0.1, 0.78, 0.8])\n",
        "    cax.get_xaxis().set_visible(False)\n",
        "    cax.get_yaxis().set_visible(False)\n",
        "    cax.patch.set_alpha(0)\n",
        "    cax.set_frame_on(False)\n",
        "    plt.colorbar(orientation='vertical')\n",
        "    plt.show()\n",
        "    \n",
        "      \n",
        "def report2dict(cr):\n",
        "    # Parse rows\n",
        "    tmp = list()\n",
        "    for row in cr.split(\"\\n\"):\n",
        "        parsed_row = [x for x in row.split(\"  \") if len(x) > 0]\n",
        "        if len(parsed_row) > 0:\n",
        "            tmp.append(parsed_row)\n",
        "    \n",
        "    # Store in dictionary\n",
        "    measures = tmp[0]\n",
        "\n",
        "    D_class_data = defaultdict(dict)\n",
        "    for row in tmp[1:]:\n",
        "        class_label = row[0]\n",
        "        for j, m in enumerate(measures):\n",
        "            D_class_data[class_label][m.strip()] = float(row[j + 1].strip())\n",
        "    return pd.DataFrame(D_class_data).T\n",
        "\n",
        "def predictAndPrint(pl):\n",
        "    print(\"acc\",accuracy_score(true_labels,pl))\n",
        "#     print(precision_recall_fscore_support(true_labels,pl,average='macro'))\n",
        "    print(confusion_matrix(true_labels,pl))\n",
        "#     draw2DArray(confusion_matrix(gold_labels_dev,pl))\n",
        "    return report2dict(classification_report(true_labels, pl))# target_names=class_names))\n",
        "    \n",
        "\n",
        "\n",
        "def drawPRcurve(y_test,y_score,it_no):\n",
        "    \n",
        "    fig = plt.figure()\n",
        "    splt = fig.add_subplot(111)\n",
        "    \n",
        "\n",
        "    precision, recall, _ = precision_recall_curve(y_test, y_score,pos_label=1)\n",
        "\n",
        "    splt.step(recall, precision, color='b', alpha=0.2,\n",
        "             where='post')\n",
        "    splt.fill_between(recall, precision, step='post', alpha=0.2,\n",
        "                     color='b')\n",
        "    average_precision = average_precision_score(y_test, y_score)\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlim([0.0, 1.05])\n",
        "    plt.title('{0:d} Precision-Recall curve: AP={1:0.2f}'.format(it_no,\n",
        "              average_precision))\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6QHWoxv_D_WK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "eac9835f-799b-48be-9c66-f84da5ec93ab"
      },
      "cell_type": "code",
      "source": [
        "#load dataset\n",
        "test_L_S = np.load(\"test_synthetic_smooth.npy\")\n",
        "train_L_S = np.load(\"train_synthetic_smooth.npy\")\n",
        "dev_L_S = test_L_S\n",
        "\n",
        "gold_labels_test = test_L_S[:,0,-1]\n",
        "gold_labels_dev = gold_labels_test\n",
        "print('shape', gold_labels_test.shape)\n",
        "\n",
        "test_L_S = np.delete(test_L_S, -1, 2)#test_L_S[:][:][:-1]\n",
        "train_L_S = np.delete(train_L_S, -1, 2)#train_L_S[:][:][:-1]\n",
        "dev_L_S = np.delete(dev_L_S, -1, 2)#dev_L_S[:][:][:-1]\n",
        "\n",
        "print(test_L_S.shape,train_L_S.shape)\n",
        "NoOfClasses=2\n",
        "print(NoOfClasses)\n",
        "NoOfLFs = test_L_S.shape[2]\n",
        "print(NoOfLFs)\n",
        "\n",
        "# test_L_S[test_L_S[:, 0, 9] == LF_l[9],0,9]\n",
        "# LF_names= [lf.__name__ for lf in LFs]\n",
        "print(train_L_S[0,:,:10])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "shape (200,)\n",
            "(200, 2, 50) (800, 2, 50)\n",
            "2\n",
            "50\n",
            "[[1.         1.         1.         0.         1.         1.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.45367731 0.44655012 0.88480848 0.00816041 0.59026071 0.76577935\n",
            "  0.11172053 0.52782908 0.7324404  0.30020824]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "adVnw_DgD_Wl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "54394929-392e-4fb9-d287-43ab88466868"
      },
      "cell_type": "code",
      "source": [
        "sys.stdout = orig_stdout\n",
        "print('test')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NSeSN3crD_Wx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#added new model\n",
        "def train2(lr,ep,th,af,batch_size=32,LF_acc=None,LF_rec=None,pcl=np.array([-1,1],dtype=np.float64),norm=True,\\\n",
        "          smooth=True,penalty=0,p3k=3,alp=1,Gamma=1.0,debug=True, \\\n",
        "           r1=1.0, r2=1.0, a_pr=None, n_pr=None, user_a = None, isdiscrete = None):\n",
        "    \n",
        "    ## lr : learning rate\n",
        "    ## ep : no of epochs\n",
        "    ## th : thetas initializer\n",
        "    ## af : alphas initializer\n",
        "    ## penalty : {1,2,3} use one of the three penalties, 0: no-penalty\n",
        "    ## p3k : parameter for penalty-3 \n",
        "    ## smooth : flag if smooth lfs are used \n",
        "    ## make sure smooth/discrete LF data is loaded into train_L_S and test_L_S\n",
        "    ## pcl : all possible class labels  = [-1,1] for binary, \n",
        "    ##       np.arange(0,NoOfClasses) for multiclass\n",
        "    ## alp : alpha parameter (to set a max value for alpha)\n",
        "    ## norm : use normalization or not\n",
        "    ## Gamma : penalty tuning parameter\n",
        "    \n",
        "    BATCH_SIZE = batch_size\n",
        "    tf.reset_default_graph()\n",
        "\n",
        "    seed = 12\n",
        "    \n",
        "    LF_k = []\n",
        "    if(len(pcl)==2):\n",
        "        for i in range(len(LF_l)):\n",
        "            LF_k.append(int((LF_l[i]+1)/2))\n",
        "    with tf.Graph().as_default():\n",
        "        xtra = tf.Variable(0, trainable=False)\n",
        "        gammaR = tf.constant(0, dtype=tf.float64)\n",
        "\n",
        "        train_dataset = tf.data.Dataset.from_tensor_slices(train_L_S).batch(BATCH_SIZE)\n",
        "        dev_dataset = tf.data.Dataset.from_tensor_slices(dev_L_S).batch(len(dev_L_S))\n",
        "        test_dataset = tf.data.Dataset.from_tensor_slices(test_L_S).batch(len(test_L_S))\n",
        "\n",
        "        iterator = tf.data.Iterator.from_structure(train_dataset.output_types, train_dataset.output_shapes)\n",
        "        next_element = iterator.get_next()\n",
        "\n",
        "        train_init_op = iterator.make_initializer(train_dataset)\n",
        "        dev_init_op = iterator.make_initializer(dev_dataset)\n",
        "        test_init_op = iterator.make_initializer(test_dataset)\n",
        "\n",
        "        next_element = iterator.get_next()\n",
        "\n",
        "        alphas = tf.get_variable('alphas', [NoOfLFs], initializer=af, dtype=tf.float64)\n",
        "        if(penalty in [103, 113]):\n",
        "            alphas = tf.stop_gradient(alphas*0)\n",
        "            \n",
        "        thetas = tf.get_variable('thetas', [1,NoOfLFs], initializer=th, dtype=tf.float64)\n",
        "        \n",
        "        LF_label = tf.convert_to_tensor(LF_k, dtype=tf.int32)\n",
        "        k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
        "        g = tf.convert_to_tensor(Gamma, dtype=tf.float64)\n",
        "        \n",
        "        if(penalty in [4,5,7,8,9,10,11, 44, 42, 41]):\n",
        "            LF_a = tf.convert_to_tensor(LF_acc, dtype=tf.float64)\n",
        "        \n",
        "        if(penalty in [6,7,11,12,15]):\n",
        "            LF_r = tf.convert_to_tensor(LF_rec, dtype=tf.float64)\n",
        "        \n",
        "        l,s =  tf.unstack(next_element,axis=1)\n",
        "\n",
        "        if(smooth and penalty < 100):\n",
        "            s_ = tf.maximum(tf.subtract(s,tf.minimum(alphas,alp)), 0)\n",
        "            if(debug):\n",
        "                print(\"s_\",s_)\n",
        "\n",
        "        def iskequalsy(v,s):\n",
        "            out = tf.where(tf.equal(v,s),tf.ones_like(v),-tf.ones_like(v))\n",
        "            if(debug):\n",
        "                print(\"out\",out)\n",
        "            return out\n",
        "#MAP\n",
        "        if(penalty < 100):\n",
        "            if(smooth):\n",
        "                pout = tf.map_fn(lambda c: l*c*s_ ,pcl,name=\"pout\")\n",
        "            else:\n",
        "                pout = tf.map_fn(lambda c: l*c ,pcl,name=\"pout\")\n",
        "\n",
        "            t =  tf.squeeze(thetas)                \n",
        "                \n",
        "            def ints(y): # called for y=1 and y=-1\n",
        "                ky = iskequalsy(k,y)  # ky = 1 if k==y else -1\n",
        "                if(debug):\n",
        "                    print(\"ky\",ky)\n",
        "                out1 = alphas+((tf.exp((t*ky*(1-alphas)))-1)/(t*ky))\n",
        "                if(debug):\n",
        "                    print(\"intsy\",out1)\n",
        "                return out1                \n",
        "                \n",
        "            if(smooth):\n",
        "                #smooth normalizer\n",
        "                zy = tf.map_fn(lambda y: tf.reduce_prod(1+ints(y),axis=0),\\\n",
        "                               pcl,name=\"zy\")\n",
        "            else:\n",
        "                #discrete normalizer\n",
        "                zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t*iskequalsy(k,y)),axis=0),\\\n",
        "                               pcl,name=\"zy\")            \n",
        "#new model\n",
        "\n",
        "        if(penalty > 200):\n",
        "\n",
        "#             z_y_new = [tf.reduce_prod(1 + 1/(-1*k*thetas) * (tf.exp(-1*k*thetas) - tf.exp(-1*k*thetas*user_alphas)) ), \\\n",
        "#                    tf.reduce_prod(1 + 1/(1*k*thetas) * (tf.exp(1*k*thetas) - tf.exp(1*k*thetas*user_alphas)) ) ]\n",
        "            numbins = tf.constant(10, dtype=tf.float64)\n",
        "            #TODO: 2 binned integration over the s values to allow stable gradients.\n",
        "            sbin_widths = (1-user_alphas)/numbins\n",
        "            sbins = tf.einsum('i,j->ij', sbin_widths, tf.cast(tf.range(0,numbins+1), dtype=tf.float64))+tf.expand_dims(user_alphas,1)\n",
        "            sbins = tf.transpose(sbins)\n",
        "            \n",
        "            def pot(s):\n",
        "                if(penalty%10 == 1):\n",
        "                    return thetas*s-alphas\n",
        "                elif(penalty%10 == 2):\n",
        "                    return thetas*tf.clip_by_value(s-alphas,0,1)\n",
        "                elif(penalty%10 == 3):\n",
        "                    return thetas*s\n",
        "                return 0\n",
        "                                       \n",
        "            # over the y-s it is okay to use a map function, but not over the LFs.\n",
        "            z_y_new = tf.map_fn(lambda y: tf.reduce_prod(1 + tf.reduce_sum(tf.exp(-y*k*pot(sbins)), axis=0)), tf.constant([1,-1], dtype=tf.float64))\n",
        "#             z_y_new = tf.reduce_sum(tf.exp(-1*k*thetas*sbins), axis=0)*sbin_widths/(1-user_alphas)\n",
        "# *sbin_widths\n",
        "\n",
        "            Z_new = tf.reduce_sum(z_y_new)\n",
        "            print(\"new z \", Z_new)\n",
        "\n",
        "#p theta without exp [y x i]\n",
        "            # snap s_ to the nearest bin. \n",
        "            s_ = tf.round((s_-user_alphas)*numbins/(1-user_alphas))*sbin_widths + user_alphas\n",
        "            logp_theta_new_t = tf.map_fn(lambda y:  tf.reduce_sum(-y*l*pot(s_), axis=1), tf.constant([1,-1], dtype=tf.float64))\n",
        "\n",
        "\n",
        "            loss_new_t = tf.reduce_logsumexp(logp_theta_new_t, axis=0) - tf.log(Z_new)\n",
        "            loss_new = tf.negative(tf.reduce_sum(loss_new_t))\n",
        "        \n",
        "#marginals as softmax\n",
        "\n",
        "            marginals_new = tf.expand_dims(tf.nn.softmax(logp_theta_new_t, axis=0), 2)\n",
        "\n",
        "        if(penalty < 100):\n",
        "            \n",
        "            if(debug):\n",
        "                print(\"pout\",pout)    \n",
        "    #MAP\n",
        "            t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout,\\\n",
        "                               name=\"t_pout\")\n",
        "\n",
        "            if(debug):\n",
        "                print(\"t_pout\",t_pout)\n",
        "\n",
        "        t =  tf.squeeze(thetas)\n",
        "        if(debug):\n",
        "            print(\"t\",t)\n",
        "\n",
        "        if(penalty < 100):\n",
        "            marginals = tf.nn.softmax(t_pout,axis=0)\n",
        "#         else:\n",
        "#             marginals = marginals_new\n",
        "        \n",
        "            if(debug):\n",
        "                print(\"zy\",zy)\n",
        "            logz = tf.log(tf.reduce_sum(zy,axis=0),name=\"logz\")\n",
        "            if(debug):\n",
        "                print(\"logz\",logz)\n",
        "    ##         tf.summary.scalar('logz', logz)\n",
        "            lsp = tf.reduce_logsumexp(t_pout,axis=0)\n",
        "            if(debug):\n",
        "                print(\"lsp\",lsp)\n",
        "##         tf.summary.scalar('lsp', tf.reduce_sum(lsp))\n",
        "        \n",
        "    \n",
        "        if(penalty == 20 or penalty > 100):\n",
        "            a_t = tf.convert_to_tensor(a_pr, dtype=tf.float64)\n",
        "            n_t = tf.convert_to_tensor(n_pr, dtype=tf.float64)\n",
        "\n",
        "        def get_loss_20():\n",
        "#NEW MODEL\n",
        "\n",
        "            # unnormalized_marginals [numY]\n",
        "            unnormalized_marginals = [tf.reduce_prod(tf.exp(-thetas*LF_l) + 1), \\\n",
        "                                      tf.reduce_prod(tf.exp(thetas*LF_l) + 1)]\n",
        "            # same for continuous, just use the new formulation with sbins for summation, no intergration\n",
        "            Z = tf.reduce_sum(unnormalized_marginals)\n",
        "\n",
        "            # unnormalized_marginals_tiled: [numLF, numY]\n",
        "            # per-LF-marginals: [numLFs]\n",
        "            per_LF_marginals = tf.gather(unnormalized_marginals, LF_label)\n",
        "\n",
        "#             return per_LF_marginals\n",
        "            # PÎ¸(kj=y): [numLFs]  (computing Eq 14)                       \n",
        "            per_LF_agreement_prob = tf.exp(thetas)/(1 + tf.exp(thetas))*per_LF_marginals/Z\n",
        "\n",
        "#             p_thetas =  tf.expand_dims(prod__,1) * exp_terms/(1+exp_terms) / Z\n",
        "            p_thetas = per_LF_agreement_prob\n",
        "\n",
        "            ptheta_terms = a_t * n_t * tf.log(p_thetas) + (1-a_t) * n_t * tf.log(1-p_thetas)\n",
        "                                                   \n",
        "#-------------------------------\n",
        "\n",
        "            return tf.negative(tf.reduce_sum(ptheta_terms)) # add other loss component\n",
        "\n",
        "\n",
        "        if(penalty > 100):\n",
        "            numYs=2\n",
        "            per_lf_prob, loss_new, marginals = model(numYs, k, l, s, thetas, alphas, isdiscrete, user_a, penalty)\n",
        "            ptheta_ = precision_loss(a_t, n_t, per_lf_prob)\n",
        "            loss_precision = tf.negative(tf.reduce_sum(ptheta_))\n",
        "            \n",
        "#             xyz = sbin_widths\n",
        "                       \n",
        "            \n",
        "  \n",
        "        prec_loss = tf.constant(0)\n",
        "    \n",
        "        if(not norm):\n",
        "            print(\"unnormlized loss\")\n",
        "            loss = tf.negative(tf.reduce_sum(lsp  ))\n",
        "        elif(penalty == 1):\n",
        "            print(\"penalty1\")\n",
        "            loss = tf.negative(tf.reduce_sum(lsp  - logz  )) \\\n",
        "                      +(g*tf.reduce_sum(tf.maximum(tf.zeros_like(thetas),-thetas)))\n",
        "        elif(penalty == 2):\n",
        "            print(\"penalty2\")\n",
        "            loss = tf.negative(tf.reduce_sum(lsp  - logz  )) \\\n",
        "                     -(g*tf.minimum( tf.reduce_min(thetas),0.0))\n",
        "            prec_loss = tf.constant(0)\n",
        "        elif(penalty == 3):\n",
        "            print(\"penalty3\")\n",
        "            loss = tf.negative(tf.reduce_sum(lsp  - logz  )) \\\n",
        "                     +(g*tf.reduce_sum(tf.log(1+tf.exp(-thetas-p3k))))\n",
        "        \n",
        "\n",
        "        elif(penalty == 4):\n",
        "            print(\"precision penalty\")\n",
        "            prec_loss = tf.reduce_sum(tf.map_fn(lambda j: softplus_p(j),np.arange(NoOfLFs),\\\n",
        "                                             dtype=tf.float64))\n",
        "            loss = tf.negative(tf.reduce_sum(lsp  - logz  )) \\\n",
        "                + (g* prec_loss)\n",
        "        \n",
        "        elif(penalty == 41):\n",
        "            print(\"precision penalty\")\n",
        "            prec_loss, xtra = sfp()\n",
        "            loss = tf.negative(tf.reduce_sum(lsp  - logz  )) \\\n",
        "                + (g * prec_loss)\n",
        "#                 + (g*tf.reduce_sum(tf.map_fn(lambda j: softplus_p(j),np.arange(NoOfLFs),\\\n",
        "#                                              dtype=tf.float64)))\n",
        "        \n",
        "        elif(penalty == 42):\n",
        "            print(\"precision penalty\")\n",
        "            temp_prloss, _ = sfpp()\n",
        "            temp_uloss = tf.negative(tf.reduce_sum(lsp  - logz  ))\n",
        "            r21 = tf.gradients(temp_uloss, thetas)\n",
        "            r22 = tf.gradients(temp_prloss, thetas)\n",
        "            xtra_tfg = tf.reduce_sum(tf.abs(r21[0][0]))/tf.reduce_sum(tf.abs(r22[0][0]))\n",
        "#             gammaR = temp_uloss/temp_prloss\n",
        "#             prec_loss, xtra = sfpp()\n",
        "            prec_loss = tf.reduce_sum(temp_prloss)\n",
        "\n",
        "            grad_ratio = 1\n",
        "\n",
        "            xtra = [xtra_tfg]\n",
        "\n",
        "            loss = temp_uloss \\\n",
        "                + (g * prec_loss)\n",
        "\n",
        "        elif(penalty == 44):\n",
        "            print(\"precision penalty\")\n",
        "            loss = tf.negative(tf.reduce_sum(lsp  - logz  )) \\\n",
        "                + (g*tf.reduce_sum(tf.map_fn(lambda j: softplus_pp(j),np.arange(NoOfLFs),\\\n",
        "                                             dtype=tf.float64)))\n",
        "                \n",
        "                \n",
        "        elif(penalty == 5):\n",
        "            print(\"precision log(softplus) penalty\")\n",
        "            loss = tf.negative(tf.reduce_sum(lsp  - logz  )) \\\n",
        "                + (g*tf.reduce_sum(tf.map_fn(lambda j: tf.log(softplus_p(j)),np.arange(NoOfLFs),\\\n",
        "                                             dtype=tf.float64)))\n",
        "        elif(penalty == 6):\n",
        "            print(\"recall penalty\")\n",
        "            loss = tf.negative(tf.reduce_sum(lsp  - logz  )) \\\n",
        "                + (g*tf.reduce_sum(tf.map_fn(lambda j: softplus_r(j),np.arange(NoOfLFs),\\\n",
        "                                             dtype=tf.float64)))\n",
        "        elif(penalty == 7):\n",
        "            print(\"precision and recall penalty\")\n",
        "            loss = tf.negative(tf.reduce_sum(lsp  - logz  )) \\\n",
        "                + (g*tf.reduce_sum(tf.map_fn(lambda j: softplus_p(j),np.arange(NoOfLFs),\\\n",
        "                                             dtype=tf.float64))) \\\n",
        "                + (g*tf.reduce_sum(tf.map_fn(lambda j: softplus_r(j),np.arange(NoOfLFs),\\\n",
        "                                             dtype=tf.float64)))\n",
        "        elif(penalty == 8):\n",
        "            print(\"precision and sign 1 penalty\")\n",
        "            loss = tf.negative(tf.reduce_sum(lsp  - logz  )) \\\n",
        "                + (g*tf.reduce_sum(tf.map_fn(lambda j: softplus_p(j),np.arange(NoOfLFs),\\\n",
        "                                             dtype=tf.float64))) \\\n",
        "                + (g*tf.reduce_sum(tf.maximum(tf.zeros_like(thetas),-thetas)))\n",
        "        elif(penalty == 9):\n",
        "            print(\"precision and sign 2 penalty\")\n",
        "            loss = tf.negative(tf.reduce_sum(lsp  - logz  )) \\\n",
        "                + (g*tf.reduce_sum(tf.map_fn(lambda j: softplus_p(j),np.arange(NoOfLFs),\\\n",
        "                                             dtype=tf.float64))) \\\n",
        "                - (g*tf.minimum( tf.reduce_min(thetas),0.0))\n",
        "        elif(penalty == 10):\n",
        "            print(\"precision and sign 3 penalty\")\n",
        "            loss = tf.negative(tf.reduce_sum(lsp  - logz  )) \\\n",
        "                + (g*tf.reduce_sum(tf.map_fn(lambda j: softplus_p(j),np.arange(NoOfLFs),\\\n",
        "                                             dtype=tf.float64))) \\\n",
        "                + (g*tf.reduce_sum(tf.log(1+tf.exp(-thetas-p3k))))\n",
        "        elif(penalty == 11):\n",
        "            print(\"precision and recall and sign 2 penalty\")\n",
        "            loss = tf.negative(tf.reduce_sum(lsp  - logz  )) \\\n",
        "                + (g*tf.reduce_sum(tf.map_fn(lambda j: softplus_p(j),np.arange(NoOfLFs),\\\n",
        "                                             dtype=tf.float64))) \\\n",
        "                + (g*tf.reduce_sum(tf.map_fn(lambda j: softplus_r(j),np.arange(NoOfLFs),\\\n",
        "                                             dtype=tf.float64))) \\\n",
        "                  - (g*tf.minimum( tf.reduce_min(thetas),0.0))\n",
        "        elif(penalty == 12):\n",
        "            print(\"recall and sign 2 penalty\")\n",
        "            loss = tf.negative(tf.reduce_sum(lsp  - logz  )) \\\n",
        "                + (g*tf.reduce_sum(tf.map_fn(lambda j: softplus_r(j),np.arange(NoOfLFs),\\\n",
        "                                             dtype=tf.float64))) \\\n",
        "                  - (g*tf.minimum( tf.reduce_min(thetas),0.0))\n",
        "        elif(penalty == 15):\n",
        "            print(\"equation 15 and sign 2 penalty\")\n",
        "            loss = tf.negative(tf.reduce_sum(lsp  - logz  )) \\\n",
        "                + (g*tf.reduce_sum(tf.map_fn(lambda j: softplus_r15(j),np.arange(NoOfLFs),\\\n",
        "                                             dtype=tf.float64))) \\\n",
        "                  - (g*tf.minimum( tf.reduce_min(thetas),0.0))\n",
        "\n",
        "\n",
        "\n",
        "                    \n",
        "        elif(penalty == 20):\n",
        "            print(\"new model\")\n",
        "            prec_loss = get_loss_20()\n",
        "            loss =  tf.negative(tf.reduce_sum(lsp  - logz  )) + prec_loss\n",
        "                  \n",
        "        elif(penalty >100 and penalty<111):\n",
        "            sys.stdout = stdout_saved\n",
        "            print(\"new model smooth \", penalty)\n",
        "            sys.stdout = open(\"trash\", \"w\")            \n",
        "            print(\"new model smooth \", penalty)\n",
        "            loss = loss_new\n",
        "#             loss, _ = smooth_new_prec()\n",
        "            \n",
        "                    \n",
        "        elif(penalty >110):\n",
        "            sys.stdout = stdout_saved\n",
        "            print(\"new model smooth with constraints \", penalty)\n",
        "            sys.stdout = open(\"trash\", \"w\")            \n",
        "            print(\"new model smooth with constraints \", penalty)\n",
        "            loss = loss_new + loss_precision\n",
        "#             loss_un, loss_pr = smooth_new_prec()\n",
        "#             loss = loss_un + loss_pr\n",
        "\n",
        "        else:\n",
        "            print(\"normalized loss\")\n",
        "            loss = tf.negative(tf.reduce_sum(lsp  - logz  ))\n",
        "            prec_loss = tf.constant(0)\n",
        "       \n",
        "        if(debug):\n",
        "            print(\"loss\",loss)\n",
        "\n",
        "#         marginals = tf.nn.softmax(t_pout,axis=0)\n",
        "\n",
        "        if(debug):\n",
        "            print(\"marginals\",marginals)\n",
        "        predict = tf.argmax(marginals,axis=0)\n",
        "\n",
        "\n",
        "        train_step = tf.train.AdamOptimizer(lr).minimize(loss)#, var_list=[thetas, alphas]) \n",
        "\n",
        "\n",
        "        init_g = tf.global_variables_initializer()\n",
        "        init_l = tf.local_variables_initializer()\n",
        "        \n",
        "        dev_high = np.zeros(3)\n",
        "        test_high = np.zeros(3)\n",
        "        \n",
        "        with tf.Session() as sess:\n",
        "            sess.run(init_g)\n",
        "            sess.run(init_l)\n",
        "\n",
        "            flg = False\n",
        "            # Initialize an iterator over the training dataset.\n",
        "            for en in range(ep):\n",
        "                sess.run(train_init_op)\n",
        "#                 temprun = sess.run(next_element2)\n",
        "#                 print('len next ',len(temprun), len(temprun[0]))\n",
        "#                 print(temprun[0])\n",
        "                tl = 0\n",
        "                pll = 0\n",
        "                unl = 0\n",
        "                try:\n",
        "                    it = 0\n",
        "                    \n",
        "                    grad_sum = 0.0\n",
        "                    while True:\n",
        "#                         print(en, it)\n",
        "                        \n",
        "                        _,ls,ploss,xpp,t = sess.run([train_step,loss,prec_loss,xtra,thetas])\n",
        "                        if(penalty == 42):\n",
        "                            un_loss = sess.run(temp_uloss)\n",
        "\n",
        "                        tl = tl + ls\n",
        "                        pll = pll + ploss\n",
        "                        if(penalty == 42):\n",
        "                            unl = unl + un_loss\n",
        "                        it = it + 1\n",
        "#                         newptheta = sess.run(sbins)\n",
        "#                         ztemp = sess.run(xyz)\n",
        "                \n",
        "#                         print(it*BATCH_SIZE)\n",
        "                \n",
        "                except tf.errors.OutOfRangeError:\n",
        "                    pass\n",
        "                opstring = \"\"\n",
        "                print(en,\"loss\",tl)\n",
        "                opstring = opstring + str(en) + \",\" + str(tl) + \",\"\n",
        "#                 print('temp__', temp__)\n",
        "                print(\"un loss\", unl)\n",
        "                print(\"ploss\", pll)\n",
        "                print(\"ratio\", unl/pll)\n",
        "                print(\"extra\", xpp)\n",
        "#                 dloss = sess.run(dev_loss)\n",
        "#                 print(\"dloss\", dloss)\n",
        "#                 print(\"expected ratio\", xpp/(xpp+1))\n",
        "                print(\"dev set\")\n",
        "                sess.run(dev_init_op)\n",
        "##                 sm,a,t,m,pl = sess.run([summary_merged,alphas,thetas,marginals,predict])\n",
        "#                 a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
        "                a,t,m,pl,ddloss = sess.run([alphas,thetas,marginals,predict,loss])\n",
        "##                 test_writer.add_summary(sm, en)\n",
        "                print('init dev loss', ddloss)\n",
        "                opstring = opstring + str(ddloss) + \",\"\n",
        "                print(a)\n",
        "                print(t)\n",
        "                unique, counts = np.unique(pl, return_counts=True)\n",
        "                print(dict(zip(unique, counts)))\n",
        "                print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
        "                dev_results = precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\")\n",
        "                print('devresults ', dev_results)\n",
        "                roc_dev = roc_auc_score(np.array(gold_labels_dev), np.array(m[1,:,0]))\n",
        "                print('roc score', roc_dev)\n",
        "                opstring = str(en) + ',' #do not print losses\n",
        "                opstring = opstring + str(dev_results[2]) + \",\" + str(roc_dev) + \",\"\n",
        "#                 if(dev_results[2]>dev_high[2]):\n",
        "                dev_high = np.array(dev_results)\n",
        "#                 dev_high = max(dev_high, dev_results[2])\n",
        "                print()\n",
        "                print(\"test set\")\n",
        "                sess.run(test_init_op)\n",
        "                a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
        "                unique, counts = np.unique(pl, return_counts=True)\n",
        "                print(dict(zip(unique, counts)))\n",
        "                print(\"acc\",accuracy_score(gold_labels_test,pl))\n",
        "                test_results = precision_recall_fscore_support(np.array(gold_labels_test),np.array(pl),average=\"binary\")\n",
        "                print('testresults ', test_results)\n",
        "                roc_test = roc_auc_score(np.array(gold_labels_test), np.array(m[1,:,0]))\n",
        "                print('roc score', roc_test)\n",
        "                opstring = opstring + str(test_results[2]) + \",\" + str(roc_test)\n",
        "#                 if(test_results[2]>test_high[2]):\n",
        "                test_high = np.array(test_results)\n",
        "                print()\n",
        "        \n",
        "                sys.stdout = stdout_saved\n",
        "                print(opstring)\n",
        "# #                 newptheta = sess.run(marginals)\n",
        "#                 print(\"new z \", ztemp)\n",
        "#                 print(\"new z shape \", np.array(ztemp).shape)\n",
        "#                 print('shape ',np.array(newptheta).shape)\n",
        "                sys.stdout = open(\"trash\", \"w\")\n",
        "                \n",
        "\n",
        "    return pl, m, dev_high, opstring#test_high"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "LoXrL3JYD_Wa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1072
        },
        "outputId": "e1959952-88a6-48b1-fc29-15f07d6b22a6"
      },
      "cell_type": "code",
      "source": [
        "#added 2\n",
        "r42d = []\n",
        "r42t = []\n",
        "\n",
        "\n",
        "a = 0.85\n",
        "ua = np.repeat(a, len(LF_l))\n",
        "dscr = np.repeat(0, len(LF_l))\n",
        "print(LF_l, len(LF_l), ua)\n",
        "# import sys\n",
        "# stdout_saved = sys.stdout\n",
        "stdout_saved = orig_stdout\n",
        "# for rx in [1, len(LF_l), len(train_L_S)]:\n",
        "# for i in np.linspace(1,1,1):\n",
        "b = 32\n",
        "for pen_i in [101,102,103,104,105,106,111,112,113,114,115,116]:#[0, 4, 42]:\n",
        "# for b in [32]:#[32,64,128,512,1024,2048,4096]:\n",
        "    devmax = np.zeros(3)\n",
        "    testmax = np.zeros(3)\n",
        "    for gg in range(1):#[1]:#np.linspace(0,100,101):#, 10, 30, 50, 100]:\n",
        "#         print(\"batch-size:\",b, \"gamma:\",gg)\n",
        "        print('============', pen_i)\n",
        "        \n",
        "        sys.stdout = open('trash', 'w')\n",
        "#        sys.stdout = stdout_saved\n",
        "        \n",
        "        _, _, _, ops = train2(1/len(train_L_S),200,batch_size = b, th = tf.truncated_normal_initializer(0,0.1,seed),\\\n",
        "                                    af = tf.truncated_normal_initializer(0,0.1,seed),\\\n",
        "                                    user_a = ua,\\\n",
        "                                    LF_acc = np.repeat(0.4, len(LF_l)) ,pcl=np.array([-1,1],dtype=np.float64),\\\n",
        "                                    norm=True,smooth=True,penalty=pen_i, Gamma=gg, debug=False,\\\n",
        "                                    a_pr = np.repeat(0.75, len(LF_l)), \\\n",
        "                                       n_pr = np.repeat(600, len(LF_l)),\\\n",
        "                                     isdiscrete = dscr)\n",
        "        sys.stdout = orig_stdout\n",
        "# get_LF_acc(dev_L_S,gold_labels_dev)\n",
        "#                 if(devmax_t[2]>devmax[2]):\n",
        "#         devmax = np.array(devmax_t)\n",
        "#                 if(testmax_t[2]>testmax[2]):\n",
        "#         testmax = np.array(testmax_t)\n",
        "#         r42d.append(devmax)\n",
        "#         r42t.append(testmax)\n",
        "#         print(\"\\n\\nfinal_result_line: \",\"thetas:\",0,0.1,\"batch size\",b,\"penalty\",pen_i,)\n",
        "##        sys.stdout = stdout_saved\n",
        "#         print(\"\\n\\nfinal_result_line: \",\"thetas:\",0,0.1,\"batch size\",b,\"penalty\",pen_i,)\n",
        "# sys.stdout = open('opfile2', 'a')\n",
        "# print('rate = 1/train_len')      \n",
        "# sys.stdout = stdout_saved"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-2ab8cc9fb717>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m#        sys.stdout = stdout_saved\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_L_S\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtruncated_normal_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m                                    \u001b[0maf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtruncated_normal_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m                                    \u001b[0muser_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mua\u001b[0m\u001b[0;34m,\u001b[0m                                    \u001b[0mLF_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLF_l\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mpcl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m                                    \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msmooth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpen_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebu...\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0morig_stdout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# get_LF_acc(dev_L_S,gold_labels_dev)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-97c8016d3d1b>\u001b[0m in \u001b[0;36mtrain2\u001b[0;34m(lr, ep, th, af, batch_size, LF_acc, LF_rec, pcl, norm, smooth, penalty, p3k, alp, Gamma, debug, r1, r2, a_pr, n_pr, user_a, isdiscrete)\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[0;31m#                         print(en, it)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m                         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mls\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mploss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxpp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprec_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxtra\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mthetas\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m                         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpenalty\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m                             \u001b[0mun_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_uloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "3lfMfe0lD_W6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}