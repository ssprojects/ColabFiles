{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#call this only once for a kernel startup\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import model_only\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "from numpy import random as rand\n",
    "import math\n",
    "\n",
    "\n",
    "# BATCH_SIZE = 32\n",
    "seed = 12\n",
    "\n",
    "\n",
    "orig_stdout = sys.stdout\n",
    "\n",
    "data_discrete = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gauss_sep = 0.1\n",
    "gauss_var = 0.1\n",
    "gauss_var_pos = 0.1\n",
    "discrete_threshold = 0.85\n",
    "smooth_threshold = 0.7\n",
    "threshold = [0.7, 0.85]\n",
    "explambda=0.8\n",
    "\n",
    "def gauss_plus():\n",
    "    return np.clip(np.random.normal(0.5 + gauss_sep, gauss_var), 0, 1)\n",
    "    print(gauss_plus(5))\n",
    "\n",
    "def gauss_minus():\n",
    "    return np.clip(np.random.normal(0.5 - gauss_sep, gauss_var), 0, 1)\n",
    "    print(gauss_minus(5))\n",
    "    \n",
    "def exp_plus():\n",
    "    return np.clip(np.random.exponential(explambda), 0, 1)\n",
    "\n",
    "def exp_minus():\n",
    "    return 1 - exp_plus()\n",
    "\n",
    "def half_gauss_plus():\n",
    "    val = 1-np.abs(np.random.normal(0, gauss_var_pos))\n",
    "    return np.clip(val, 0, 1)\n",
    "    print(gauss_plus(5))\n",
    "\n",
    "def half_gauss_minus():\n",
    "    return np.clip(np.abs(np.random.normal(0, gauss_var)), 0, 1)\n",
    "    print(gauss_minus(5))\n",
    "\n",
    "def gauss_random_pos():\n",
    "    prob = 0.5 + epsilon\n",
    "    sample = np.random.rand()\n",
    "    if(sample<prob):\n",
    "        return gauss_plus()\n",
    "    return gauss_minus()\n",
    "\n",
    "def gauss_random_neg():\n",
    "    prob = 0.5 - epsilon\n",
    "    sample = np.random.rand()\n",
    "    if(sample<prob):\n",
    "        return gauss_plus()\n",
    "    return gauss_minus()\n",
    "\n",
    "#discrete\n",
    "def generateAdversarial(k, is_discrete):\n",
    "    f = np.empty([2, instances])\n",
    "#     if(is_discrete):\n",
    "    if k==-1:\n",
    "        for i,x in enumerate(labels):\n",
    "#             s = gauss[int(x)]()[0]\n",
    "            l = 0\n",
    "            if(x):\n",
    "                s = gauss_plus()\n",
    "                if(s < 1-threshold[data_discrete]):\n",
    "                    l = -1\n",
    "            else:\n",
    "                s = gauss_minus()\n",
    "                if(s > threshold[data_discrete]):\n",
    "                    l = -1\n",
    "            f[0][i] = l\n",
    "            f[1][i] = s\n",
    "    else:\n",
    "        for i,x in enumerate(labels):\n",
    "#             s = gauss[int(x)]()[0]\n",
    "            l = 0\n",
    "            if(x):\n",
    "                s = gauss_plus()\n",
    "                if(s > threshold[data_discrete]):\n",
    "                    l = 1\n",
    "            else:\n",
    "                s = gauss_minus()\n",
    "                if(s < 1-threshold[data_discrete]):\n",
    "                    l = 1\n",
    "            f[0][i] = l\n",
    "            f[1][i] = s\n",
    "    return f\n",
    "\n",
    "#discrete\n",
    "def generate(k, is_discrete):\n",
    "    f = np.empty([2, instances])\n",
    "    for i,x in enumerate(labels):\n",
    "        s = half_gauss_plus() if (x) else half_gauss_minus()\n",
    "        f[0][i] = -1 if (s < 1-threshold[data_discrete] and k==-1) else 0\n",
    "        f[0][i] = 1 if (s > threshold[data_discrete] and k==1) else 0\n",
    "        f[1][i] = s\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.5888092412 7.65288924075\n",
      "[0.9794366448300913, 0.8663424365486454, 0.82296297531311779, 0.95243956998312806, 0.99209812264660902, 0.96476617772776663, 0.91310034314468225, 0.96538614222586883, 0.79729367107191906, 0.98924889310149733, 0.97296632805880356, 0.9734665655597754, 0.969702512413394, 0.84013810065425831, 0.85156730749410825, 0.95998350170982494, 0.98584845851703495, 0.90718971850205654, 0.96029663697038026, 0.92457513468839836]\n",
      "[0.20703571257127285, 0.34318866622538002, 0.39268803311754846, 0.46596470498031689, 0.24416622437070482, 0.60255170428381843, 0.26453884814690559, 0.45819896806225741, 0.090900153421499957, 0.68688337361078478, 0.15781870954096736, 0.072071349583928065, 0.45591110829201997, 1.0, 0.45113211636004319, 0.60417150170675971, 0.27835557636033248, 0.38007440310691976, 0.37841200463276103, 0.11882608237771053]\n"
     ]
    }
   ],
   "source": [
    "pos = [half_gauss_plus() for i in range(20)]\n",
    "negs =  [half_gauss_minus() for i in range(20)]\n",
    "print (np.sum(pos), np.sum(negs))\n",
    "print (pos)\n",
    "print (negs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "seed = 12\n",
    "rand.seed(12)\n",
    "\n",
    "instances = 1000\n",
    "epsilon = 0.02 # 0.50 + epsilon = correlation\n",
    "ratio = 0.3 # ones:total\n",
    "noOfFunctions = 2\n",
    "addClassPriorFunc=True\n",
    "\n",
    "correlatedInstances = math.ceil((0.5+epsilon)*instances)\n",
    "indices = np.arange(0,instances,1)\n",
    "\n",
    "numOnes = math.ceil(ratio*instances)\n",
    "numZeros = instances - numOnes\n",
    "onesArray = np.ones(numOnes)#, dtype=int)\n",
    "zerosArray = np.zeros(numZeros)#, dtype=int)\n",
    "labels = np.concatenate((onesArray, zerosArray))\n",
    "rand.shuffle(labels) #shuffle datapoints\n",
    "\n",
    "print(np.count_nonzero(labels), correlatedInstances)\n",
    "\n",
    "functions=[]\n",
    "LF = [] \n",
    "# np.concatenate((np.ones(math.ceil(noOfFunctions/2)), -1+np.zeros(math.floor(noOfFunctions/2))))\n",
    "\n",
    "if addClassPriorFunc:\n",
    "    f = np.empty([2, instances])\n",
    "    f[0] = labels # labels are assumed to be 0/1\n",
    "    f[1] = f[0]\n",
    "    functions.append(f)\n",
    "    LF.append(1)\n",
    "# print(\"before\")\n",
    "# print(labels)\n",
    "\n",
    "\n",
    "for i in range(math.ceil(noOfFunctions/2)):\n",
    "    index_i = rand.choice(indices, size = correlatedInstances, replace=False)\n",
    "#         f = [1-labels, np.random.uniform(0,0.75, len(labels))]\n",
    "    f = generate(1, data_discrete)\n",
    "#         print(f)\n",
    "    functions.append(f)\n",
    "    LF.append(1)\n",
    "\n",
    "for i in range(math.floor(noOfFunctions/2)):\n",
    "    index_i = rand.choice(indices, size = correlatedInstances, replace=False)\n",
    "#         f = [-1*labels, np.random.uniform(0, 0.75, len(labels))]\n",
    "    f = generate(-1, data_discrete)\n",
    "#         print(f)\n",
    "    functions.append(f)\n",
    "    LF.append(-1)\n",
    "functions.append([labels, labels])\n",
    "print(\"Class conditional stats\")\n",
    "for f in functions:\n",
    "    print(\"y=+1\", np.sum(f[0]*labels))\n",
    "    print(\"y=0\", np.sum(f[0]*(1-labels)))\n",
    "functions = np.transpose(np.array(functions))\n",
    "print(functions.shape)\n",
    "y=functions\n",
    "LF_l = np.array(LF)\n",
    "print(type(y[0]))\n",
    "\n",
    "# print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700 300 1000\n",
      "240 60 560 140\n",
      "(800, 2, 3)\n",
      "(200, 2, 3)\n"
     ]
    }
   ],
   "source": [
    "last = len(y[0][0])-1\n",
    "ones = [p for p in y if p[0][last]==1]\n",
    "zeros = [p for p in y if p[0][last]==0]\n",
    "\n",
    "print(len(zeros),len(ones),len(y))\n",
    "\n",
    "train_set=[]\n",
    "test_set=[]\n",
    "\n",
    "split=0.8\n",
    "\n",
    "# split into train and test\n",
    "# same distribution of 1s\n",
    "\n",
    "train_split=int(split*len(y))\n",
    "test_split=len(y)-train_split\n",
    "\n",
    "train_ones = int(split*len(ones))\n",
    "test_ones = len(ones)-train_ones\n",
    "train_zeros = train_split-train_ones\n",
    "test_zeros = test_split-test_ones\n",
    "\n",
    "print(train_ones, test_ones, train_zeros, test_zeros)\n",
    "j=0\n",
    "k=0\n",
    "for i in range(train_split):\n",
    "    if(j<i*train_ones/train_split):\n",
    "        train_set.append(ones[j])\n",
    "        j = j +1\n",
    "    else:\n",
    "        train_set.append(zeros[k])\n",
    "        k = k +1\n",
    "train_set = np.array(train_set)\n",
    "print(train_set.shape)\n",
    "# print(np.sum(train_set[:,last]), len(train_set))\n",
    "\n",
    "j2=0\n",
    "k2=0\n",
    "for i in range(test_split):\n",
    "    if(j2<i*test_ones/test_split):\n",
    "        test_set.append(ones[j])\n",
    "        j = j +1\n",
    "        j2= j2+1\n",
    "    else:\n",
    "        test_set.append(zeros[k])\n",
    "        k = k +1\n",
    "        k2= k2+1\n",
    "test_set = np.array(test_set)\n",
    "print(test_set.shape)\n",
    "rand.shuffle(train_set)\n",
    "rand.shuffle(test_set)\n",
    "# print(np.sum(test_set[:,last]), len(test_set))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save(\"train_synthetic_smooth\",train_set)\n",
    "np.save(\"test_synthetic_smooth\", test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 2, 3)\n"
     ]
    }
   ],
   "source": [
    "print(test_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200,)\n",
      "60\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  1.  0.  0.  1.  1.  0.  0.  0.\n",
      "  0.  0.]\n",
      "2 [1 1]\n"
     ]
    }
   ],
   "source": [
    "from numpy import genfromtxt\n",
    "\n",
    "temp = np.load(\"test_synthetic_smooth.npy\")\n",
    "true_labels = temp[:,0,last]\n",
    "gold_labels_dev = true_labels\n",
    "gold_labels_test = true_labels\n",
    "print(true_labels.shape)\n",
    "print(np.count_nonzero(true_labels))\n",
    "print(true_labels[:20])\n",
    "\n",
    "# mapping class labels to labeling functions\n",
    "# can consider distance functions as giving a positively correlated score to the negative class\n",
    "\n",
    "print(len(LF_l), LF_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def draw2DArray(a):\n",
    "    fig = plt.figure(figsize=(6, 3.2))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_title('colorMap')\n",
    "    plt.imshow(np.array(a))\n",
    "    ax.set_aspect('equal')\n",
    "    cax = fig.add_axes([0.12, 0.1, 0.78, 0.8])\n",
    "    cax.get_xaxis().set_visible(False)\n",
    "    cax.get_yaxis().set_visible(False)\n",
    "    cax.patch.set_alpha(0)\n",
    "    cax.set_frame_on(False)\n",
    "    plt.colorbar(orientation='vertical')\n",
    "    plt.show()\n",
    "    \n",
    "      \n",
    "def report2dict(cr):\n",
    "    # Parse rows\n",
    "    tmp = list()\n",
    "    for row in cr.split(\"\\n\"):\n",
    "        parsed_row = [x for x in row.split(\"  \") if len(x) > 0]\n",
    "        if len(parsed_row) > 0:\n",
    "            tmp.append(parsed_row)\n",
    "    \n",
    "    # Store in dictionary\n",
    "    measures = tmp[0]\n",
    "\n",
    "    D_class_data = defaultdict(dict)\n",
    "    for row in tmp[1:]:\n",
    "        class_label = row[0]\n",
    "        for j, m in enumerate(measures):\n",
    "            D_class_data[class_label][m.strip()] = float(row[j + 1].strip())\n",
    "    return pd.DataFrame(D_class_data).T\n",
    "\n",
    "def predictAndPrint(pl):\n",
    "    print(\"acc\",accuracy_score(true_labels,pl))\n",
    "#     print(precision_recall_fscore_support(true_labels,pl,average='macro'))\n",
    "    print(confusion_matrix(true_labels,pl))\n",
    "#     draw2DArray(confusion_matrix(gold_labels_dev,pl))\n",
    "    return report2dict(classification_report(true_labels, pl))# target_names=class_names))\n",
    "    \n",
    "\n",
    "\n",
    "def drawPRcurve(y_test,y_score,it_no):\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    splt = fig.add_subplot(111)\n",
    "    \n",
    "\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_score,pos_label=1)\n",
    "\n",
    "    splt.step(recall, precision, color='b', alpha=0.2,\n",
    "             where='post')\n",
    "    splt.fill_between(recall, precision, step='post', alpha=0.2,\n",
    "                     color='b')\n",
    "    average_precision = average_precision_score(y_test, y_score)\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlim([0.0, 1.05])\n",
    "    plt.title('{0:d} Precision-Recall curve: AP={1:0.2f}'.format(it_no,\n",
    "              average_precision))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 2, 2)\n"
     ]
    }
   ],
   "source": [
    "print(train_set[:,:,:-1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape (200,)\n",
      "(200, 2, 2) (800, 2, 2)\n",
      "2\n",
      "2\n",
      "[[ 0.          0.        ]\n",
      " [ 0.          0.56326526]]\n"
     ]
    }
   ],
   "source": [
    "#load dataset\n",
    "test_L_S = np.load(\"test_synthetic_smooth.npy\")\n",
    "train_L_S = np.load(\"train_synthetic_smooth.npy\")\n",
    "dev_L_S = test_L_S\n",
    "\n",
    "gold_labels_test = test_L_S[:,0,-1]\n",
    "gold_labels_dev = gold_labels_test\n",
    "print('shape', gold_labels_test.shape)\n",
    "\n",
    "test_L_S = np.delete(test_L_S, -1, 2)#test_L_S[:][:][:-1]\n",
    "train_L_S = np.delete(train_L_S, -1, 2)#train_L_S[:][:][:-1]\n",
    "dev_L_S = np.delete(dev_L_S, -1, 2)#dev_L_S[:][:][:-1]\n",
    "\n",
    "print(test_L_S.shape,train_L_S.shape)\n",
    "NoOfClasses=2\n",
    "print(NoOfClasses)\n",
    "NoOfLFs = test_L_S.shape[2]\n",
    "print(NoOfLFs)\n",
    "\n",
    "# test_L_S[test_L_S[:, 0, 9] == LF_l[9],0,9]\n",
    "# LF_names= [lf.__name__ for lf in LFs]\n",
    "print(train_L_S[0,:,:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 0.02 1\n"
     ]
    }
   ],
   "source": [
    "sys.stdout = orig_stdout\n",
    "print('test', epsilon, noOfFunctions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#added new model\n",
    "def train2(lr,ep,th,af,batch_size=32,LF_acc=None,LF_rec=None,pcl=np.array([-1,1],dtype=np.float64),norm=True,\\\n",
    "          smooth=True,penalty=0,p3k=3,alp=1,Gamma=1.0,debug=True, \\\n",
    "           r1=1.0, r2=1.0, a_pr=None, n_pr=None, user_a = None, isdiscrete = None, alpha_max = None, r_pr=None):\n",
    "    \n",
    "    ## lr : learning rate\n",
    "    ## ep : no of epochs\n",
    "    ## th : thetas initializer\n",
    "    ## af : alphas initializer\n",
    "    ## penalty : {1,2,3} use one of the three penalties, 0: no-penalty\n",
    "    ## p3k : parameter for penalty-3 \n",
    "    ## smooth : flag if smooth lfs are used \n",
    "    ## make sure smooth/discrete LF data is loaded into train_L_S and test_L_S\n",
    "    ## pcl : all possible class labels  = [-1,1] for binary, \n",
    "    ##       np.arange(0,NoOfClasses) for multiclass\n",
    "    ## alp : alpha parameter (to set a max value for alpha)\n",
    "    ## norm : use normalization or not\n",
    "    ## Gamma : penalty tuning parameter\n",
    "    \n",
    "    BATCH_SIZE = batch_size\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    seed = 12\n",
    "    \n",
    "    LF_k = []\n",
    "    if(len(pcl)==2):\n",
    "        for i in range(len(LF_l)):\n",
    "            LF_k.append(int((LF_l[i]+1)/2))\n",
    "    with tf.Graph().as_default():\n",
    "        xtra = tf.Variable(0, trainable=False)\n",
    "        gammaR = tf.constant(0, dtype=tf.float64)\n",
    "\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices(train_L_S).batch(BATCH_SIZE)\n",
    "        dev_dataset = tf.data.Dataset.from_tensor_slices(dev_L_S).batch(len(dev_L_S))\n",
    "        test_dataset = tf.data.Dataset.from_tensor_slices(test_L_S).batch(len(test_L_S))\n",
    "\n",
    "        iterator = tf.data.Iterator.from_structure(train_dataset.output_types, train_dataset.output_shapes)\n",
    "        next_element = iterator.get_next()\n",
    "\n",
    "        train_init_op = iterator.make_initializer(train_dataset)\n",
    "        dev_init_op = iterator.make_initializer(dev_dataset)\n",
    "        test_init_op = iterator.make_initializer(test_dataset)\n",
    "\n",
    "        next_element = iterator.get_next()\n",
    "\n",
    "        alphas = tf.get_variable('alphas', [NoOfLFs], initializer=af, dtype=tf.float64)\n",
    "        if(penalty in [103, 113]):\n",
    "            alphas = tf.stop_gradient(alphas*0)\n",
    "            \n",
    "        thetas = tf.get_variable('thetas', [1,NoOfLFs], initializer=th, dtype=tf.float64)\n",
    "        \n",
    "        LF_label = tf.convert_to_tensor(LF_k, dtype=tf.int32)\n",
    "        k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
    "        g = tf.convert_to_tensor(Gamma, dtype=tf.float64)\n",
    "        \n",
    "        if(penalty in [4,5,7,8,9,10,11, 44, 42, 41]):\n",
    "            LF_a = tf.convert_to_tensor(LF_acc, dtype=tf.float64)\n",
    "        \n",
    "        if(penalty in [6,7,11,12,15]):\n",
    "            LF_r = tf.convert_to_tensor(LF_rec, dtype=tf.float64)\n",
    "        \n",
    "        l,s =  tf.unstack(next_element,axis=1)\n",
    "\n",
    "        if(smooth and penalty < 100):\n",
    "            s_ = tf.maximum(tf.subtract(s,tf.minimum(alphas,alp)), 0)\n",
    "            if(debug):\n",
    "                print(\"s_\",s_)\n",
    "\n",
    "        def iskequalsy(v,s):\n",
    "            out = tf.where(tf.equal(v,s),tf.ones_like(v),-tf.ones_like(v))\n",
    "            if(debug):\n",
    "                print(\"out\",out)\n",
    "            return out\n",
    "#MAP\n",
    "        if(penalty < 100):\n",
    "            if(smooth):\n",
    "                pout = tf.map_fn(lambda c: l*c*s_ ,pcl,name=\"pout\")\n",
    "            else:\n",
    "                pout = tf.map_fn(lambda c: l*c ,pcl,name=\"pout\")\n",
    "\n",
    "            t =  tf.squeeze(thetas)                \n",
    "                \n",
    "            def ints(y): # called for y=1 and y=-1\n",
    "                ky = iskequalsy(k,y)  # ky = 1 if k==y else -1\n",
    "                if(debug):\n",
    "                    print(\"ky\",ky)\n",
    "                out1 = alphas+((tf.exp((t*ky*(1-alphas)))-1)/(t*ky))\n",
    "                if(debug):\n",
    "                    print(\"intsy\",out1)\n",
    "                return out1                \n",
    "                \n",
    "            if(smooth):\n",
    "                #smooth normalizer\n",
    "                zy = tf.map_fn(lambda y: tf.reduce_prod(1+ints(y),axis=0),\\\n",
    "                               pcl,name=\"zy\")\n",
    "            else:\n",
    "                #discrete normalizer\n",
    "                zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t*iskequalsy(k,y)),axis=0),\\\n",
    "                               pcl,name=\"zy\")            \n",
    "#new model\n",
    "\n",
    "        if(penalty > 200):\n",
    "\n",
    "#             z_y_new = [tf.reduce_prod(1 + 1/(-1*k*thetas) * (tf.exp(-1*k*thetas) - tf.exp(-1*k*thetas*user_alphas)) ), \\\n",
    "#                    tf.reduce_prod(1 + 1/(1*k*thetas) * (tf.exp(1*k*thetas) - tf.exp(1*k*thetas*user_alphas)) ) ]\n",
    "            numbins = tf.constant(10, dtype=tf.float64)\n",
    "            #TODO: 2 binned integration over the s values to allow stable gradients.\n",
    "            sbin_widths = (1-user_alphas)/numbins\n",
    "            sbins = tf.einsum('i,j->ij', sbin_widths, tf.cast(tf.range(0,numbins+1), dtype=tf.float64))+tf.expand_dims(user_alphas,1)\n",
    "            sbins = tf.transpose(sbins)\n",
    "            \n",
    "            def pot(s):\n",
    "                if(penalty%10 == 1):\n",
    "                    return thetas*s-alphas\n",
    "                elif(penalty%10 == 2):\n",
    "                    return thetas*tf.clip_by_value(s-alphas,0,1)\n",
    "                elif(penalty%10 == 3):\n",
    "                    return thetas*s\n",
    "                return 0\n",
    "                                       \n",
    "            # over the y-s it is okay to use a map function, but not over the LFs.\n",
    "            z_y_new = tf.map_fn(lambda y: tf.reduce_prod(1 + tf.reduce_sum(tf.exp(-y*k*pot(sbins)), axis=0)), tf.constant([1,-1], dtype=tf.float64))\n",
    "#             z_y_new = tf.reduce_sum(tf.exp(-1*k*thetas*sbins), axis=0)*sbin_widths/(1-user_alphas)\n",
    "# *sbin_widths\n",
    "\n",
    "            Z_new = tf.reduce_sum(z_y_new)\n",
    "            print(\"new z \", Z_new)\n",
    "\n",
    "#p theta without exp [y x i]\n",
    "            # snap s_ to the nearest bin. \n",
    "            s_ = tf.round((s_-user_alphas)*numbins/(1-user_alphas))*sbin_widths + user_alphas\n",
    "            logp_theta_new_t = tf.map_fn(lambda y:  tf.reduce_sum(-y*l*pot(s_), axis=1), tf.constant([1,-1], dtype=tf.float64))\n",
    "\n",
    "\n",
    "            loss_new_t = tf.reduce_logsumexp(logp_theta_new_t, axis=0) - tf.log(Z_new)\n",
    "            loss_new = tf.negative(tf.reduce_sum(loss_new_t))\n",
    "        \n",
    "#marginals as softmax\n",
    "\n",
    "            marginals_new = tf.expand_dims(tf.nn.softmax(logp_theta_new_t, axis=0), 2)\n",
    "\n",
    "        if(penalty < 100):\n",
    "            \n",
    "            if(debug):\n",
    "                print(\"pout\",pout)    \n",
    "    #MAP\n",
    "            t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout,\\\n",
    "                               name=\"t_pout\")\n",
    "\n",
    "            if(debug):\n",
    "                print(\"t_pout\",t_pout)\n",
    "\n",
    "        t =  tf.squeeze(thetas)\n",
    "        if(debug):\n",
    "            print(\"t\",t)\n",
    "\n",
    "        if(penalty < 100):\n",
    "            marginals = tf.nn.softmax(t_pout,axis=0)\n",
    "#         else:\n",
    "#             marginals = marginals_new\n",
    "        \n",
    "            if(debug):\n",
    "                print(\"zy\",zy)\n",
    "            logz = tf.log(tf.reduce_sum(zy,axis=0),name=\"logz\")\n",
    "            if(debug):\n",
    "                print(\"logz\",logz)\n",
    "    ##         tf.summary.scalar('logz', logz)\n",
    "            lsp = tf.reduce_logsumexp(t_pout,axis=0)\n",
    "            if(debug):\n",
    "                print(\"lsp\",lsp)\n",
    "##         tf.summary.scalar('lsp', tf.reduce_sum(lsp))\n",
    "        \n",
    "    \n",
    "        if(penalty == 20 or penalty > 100):\n",
    "            a_t = tf.convert_to_tensor(a_pr, dtype=tf.float64)\n",
    "            n_t = tf.convert_to_tensor(n_pr, dtype=tf.float64)\n",
    "            r_t = tf.convert_to_tensor(r_pr, dtype=tf.float64)\n",
    "        def get_loss_20():\n",
    "#NEW MODEL\n",
    "\n",
    "            # unnormalized_marginals [numY]\n",
    "            unnormalized_marginals = [tf.reduce_prod(tf.exp(-thetas*LF_l) + 1), \\\n",
    "                                      tf.reduce_prod(tf.exp(thetas*LF_l) + 1)]\n",
    "            # same for continuous, just use the new formulation with sbins for summation, no intergration\n",
    "            Z = tf.reduce_sum(unnormalized_marginals)\n",
    "\n",
    "            # unnormalized_marginals_tiled: [numLF, numY]\n",
    "            # per-LF-marginals: [numLFs]\n",
    "            per_LF_marginals = tf.gather(unnormalized_marginals, LF_label)\n",
    "\n",
    "#             return per_LF_marginals\n",
    "            # PÎ¸(kj=y): [numLFs]  (computing Eq 14)                       \n",
    "            per_LF_agreement_prob = tf.exp(thetas)/(1 + tf.exp(thetas))*per_LF_marginals/Z\n",
    "\n",
    "#             p_thetas =  tf.expand_dims(prod__,1) * exp_terms/(1+exp_terms) / Z\n",
    "            p_thetas = per_LF_agreement_prob\n",
    "\n",
    "            ptheta_terms = a_t * n_t * tf.log(p_thetas) + (1-a_t) * n_t * tf.log(1-p_thetas)\n",
    "                                                   \n",
    "#-------------------------------\n",
    "\n",
    "            return tf.negative(tf.reduce_sum(ptheta_terms)) # add other loss component\n",
    "\n",
    "\n",
    "        loss_recall = 0\n",
    "        if(penalty > 100):\n",
    "            numYs=2\n",
    "            loss_new, per_lf_prob, marginals, per_lf_recall, pots = model_only.model(numYs, k, l, s, thetas, alphas, isdiscrete, user_a, penalty,alpha_max)\n",
    "            loss_precision = model_only.precision_loss(a_t, n_t, per_lf_prob, penalty)\n",
    "            if r_t is not None and (penalty/10) % 10 == 3:\n",
    "                loss_recall = model_only.recall_loss(r_t, n_t, per_lf_recall, isdiscrete)\n",
    "\n",
    "        prec_loss = tf.constant(0)\n",
    "    \n",
    "        if(not norm):\n",
    "            print(\"unnormlized loss\")\n",
    "            loss = tf.negative(tf.reduce_sum(lsp  ))\n",
    "        elif(penalty == 1):\n",
    "            print(\"penalty1\")\n",
    "            loss = tf.negative(tf.reduce_sum(lsp  - logz  )) \\\n",
    "                      +(g*tf.reduce_sum(tf.maximum(tf.zeros_like(thetas),-thetas)))\n",
    "        elif(penalty == 2):\n",
    "            print(\"penalty2\")\n",
    "            loss = tf.negative(tf.reduce_sum(lsp  - logz  )) \\\n",
    "                     -(g*tf.minimum( tf.reduce_min(thetas),0.0))\n",
    "            prec_loss = tf.constant(0)\n",
    "        elif(penalty == 3):\n",
    "            print(\"penalty3\")\n",
    "            loss = tf.negative(tf.reduce_sum(lsp  - logz  )) \\\n",
    "                     +(g*tf.reduce_sum(tf.log(1+tf.exp(-thetas-p3k))))\n",
    "        \n",
    "\n",
    "        elif(penalty == 4):\n",
    "            print(\"precision penalty\")\n",
    "            prec_loss = tf.reduce_sum(tf.map_fn(lambda j: softplus_p(j),np.arange(NoOfLFs),\\\n",
    "                                             dtype=tf.float64))\n",
    "            loss = tf.negative(tf.reduce_sum(lsp  - logz  )) \\\n",
    "                + (g* prec_loss)\n",
    "        \n",
    "        elif(penalty == 41):\n",
    "            print(\"precision penalty\")\n",
    "            prec_loss, xtra = sfp()\n",
    "            loss = tf.negative(tf.reduce_sum(lsp  - logz  )) \\\n",
    "                + (g * prec_loss)\n",
    "#                 + (g*tf.reduce_sum(tf.map_fn(lambda j: softplus_p(j),np.arange(NoOfLFs),\\\n",
    "#                                              dtype=tf.float64)))\n",
    "        \n",
    "        elif(penalty == 42):\n",
    "            print(\"precision penalty\")\n",
    "            temp_prloss, _ = sfpp()\n",
    "            temp_uloss = tf.negative(tf.reduce_sum(lsp  - logz  ))\n",
    "            r21 = tf.gradients(temp_uloss, thetas)\n",
    "            r22 = tf.gradients(temp_prloss, thetas)\n",
    "            xtra_tfg = tf.reduce_sum(tf.abs(r21[0][0]))/tf.reduce_sum(tf.abs(r22[0][0]))\n",
    "#             gammaR = temp_uloss/temp_prloss\n",
    "#             prec_loss, xtra = sfpp()\n",
    "            prec_loss = tf.reduce_sum(temp_prloss)\n",
    "\n",
    "            grad_ratio = 1\n",
    "\n",
    "            xtra = [xtra_tfg]\n",
    "\n",
    "            loss = temp_uloss \\\n",
    "                + (g * prec_loss)\n",
    "\n",
    "        elif(penalty == 44):\n",
    "            print(\"precision penalty\")\n",
    "            loss = tf.negative(tf.reduce_sum(lsp  - logz  )) \\\n",
    "                + (g*tf.reduce_sum(tf.map_fn(lambda j: softplus_pp(j),np.arange(NoOfLFs),\\\n",
    "                                             dtype=tf.float64)))\n",
    "                \n",
    "                \n",
    "        elif(penalty == 5):\n",
    "            print(\"precision log(softplus) penalty\")\n",
    "            loss = tf.negative(tf.reduce_sum(lsp  - logz  )) \\\n",
    "                + (g*tf.reduce_sum(tf.map_fn(lambda j: tf.log(softplus_p(j)),np.arange(NoOfLFs),\\\n",
    "                                             dtype=tf.float64)))\n",
    "        elif(penalty == 6):\n",
    "            print(\"recall penalty\")\n",
    "            loss = tf.negative(tf.reduce_sum(lsp  - logz  )) \\\n",
    "                + (g*tf.reduce_sum(tf.map_fn(lambda j: softplus_r(j),np.arange(NoOfLFs),\\\n",
    "                                             dtype=tf.float64)))\n",
    "        elif(penalty == 7):\n",
    "            print(\"precision and recall penalty\")\n",
    "            loss = tf.negative(tf.reduce_sum(lsp  - logz  )) \\\n",
    "                + (g*tf.reduce_sum(tf.map_fn(lambda j: softplus_p(j),np.arange(NoOfLFs),\\\n",
    "                                             dtype=tf.float64))) \\\n",
    "                + (g*tf.reduce_sum(tf.map_fn(lambda j: softplus_r(j),np.arange(NoOfLFs),\\\n",
    "                                             dtype=tf.float64)))\n",
    "        elif(penalty == 8):\n",
    "            print(\"precision and sign 1 penalty\")\n",
    "            loss = tf.negative(tf.reduce_sum(lsp  - logz  )) \\\n",
    "                + (g*tf.reduce_sum(tf.map_fn(lambda j: softplus_p(j),np.arange(NoOfLFs),\\\n",
    "                                             dtype=tf.float64))) \\\n",
    "                + (g*tf.reduce_sum(tf.maximum(tf.zeros_like(thetas),-thetas)))\n",
    "        elif(penalty == 9):\n",
    "            print(\"precision and sign 2 penalty\")\n",
    "            loss = tf.negative(tf.reduce_sum(lsp  - logz  )) \\\n",
    "                + (g*tf.reduce_sum(tf.map_fn(lambda j: softplus_p(j),np.arange(NoOfLFs),\\\n",
    "                                             dtype=tf.float64))) \\\n",
    "                - (g*tf.minimum( tf.reduce_min(thetas),0.0))\n",
    "        elif(penalty == 10):\n",
    "            print(\"precision and sign 3 penalty\")\n",
    "            loss = tf.negative(tf.reduce_sum(lsp  - logz  )) \\\n",
    "                + (g*tf.reduce_sum(tf.map_fn(lambda j: softplus_p(j),np.arange(NoOfLFs),\\\n",
    "                                             dtype=tf.float64))) \\\n",
    "                + (g*tf.reduce_sum(tf.log(1+tf.exp(-thetas-p3k))))\n",
    "        elif(penalty == 11):\n",
    "            print(\"precision and recall and sign 2 penalty\")\n",
    "            loss = tf.negative(tf.reduce_sum(lsp  - logz  )) \\\n",
    "                + (g*tf.reduce_sum(tf.map_fn(lambda j: softplus_p(j),np.arange(NoOfLFs),\\\n",
    "                                             dtype=tf.float64))) \\\n",
    "                + (g*tf.reduce_sum(tf.map_fn(lambda j: softplus_r(j),np.arange(NoOfLFs),\\\n",
    "                                             dtype=tf.float64))) \\\n",
    "                  - (g*tf.minimum( tf.reduce_min(thetas),0.0))\n",
    "        elif(penalty == 12):\n",
    "            print(\"recall and sign 2 penalty\")\n",
    "            loss = tf.negative(tf.reduce_sum(lsp  - logz  )) \\\n",
    "                + (g*tf.reduce_sum(tf.map_fn(lambda j: softplus_r(j),np.arange(NoOfLFs),\\\n",
    "                                             dtype=tf.float64))) \\\n",
    "                  - (g*tf.minimum( tf.reduce_min(thetas),0.0))\n",
    "        elif(penalty == 15):\n",
    "            print(\"equation 15 and sign 2 penalty\")\n",
    "            loss = tf.negative(tf.reduce_sum(lsp  - logz  )) \\\n",
    "                + (g*tf.reduce_sum(tf.map_fn(lambda j: softplus_r15(j),np.arange(NoOfLFs),\\\n",
    "                                             dtype=tf.float64))) \\\n",
    "                  - (g*tf.minimum( tf.reduce_min(thetas),0.0))\n",
    "\n",
    "\n",
    "\n",
    "                    \n",
    "        elif(penalty == 20):\n",
    "            print(\"new model\")\n",
    "            prec_loss = get_loss_20()\n",
    "            loss =  tf.negative(tf.reduce_sum(lsp  - logz  )) + prec_loss\n",
    "                  \n",
    "        elif(penalty >100 and penalty<111):\n",
    "            sys.stdout = stdout_saved\n",
    "            print(\"new model smooth \", penalty)\n",
    "            # sys.stdout = open(\"trash\", \"w\")            \n",
    "            print(\"new model smooth \", penalty)\n",
    "            loss = loss_new\n",
    "#             loss, _ = smooth_new_prec()\n",
    "            \n",
    "                    \n",
    "        elif(penalty >110):\n",
    "            sys.stdout = stdout_saved\n",
    "            print(\"new model smooth with constraints \", penalty)\n",
    "            # sys.stdout = open(\"trash\", \"w\")            \n",
    "            print(\"new model smooth with constraints \", penalty)\n",
    "            loss = loss_new + loss_precision + loss_recall\n",
    "#             loss_un, loss_pr = smooth_new_prec()\n",
    "#             loss = loss_un + loss_pr\n",
    "\n",
    "        else:\n",
    "            print(\"normalized loss\")\n",
    "            loss = tf.negative(tf.reduce_sum(lsp  - logz  ))\n",
    "            prec_loss = tf.constant(0)\n",
    "       \n",
    "        if(debug):\n",
    "            print(\"loss\",loss)\n",
    "\n",
    "#         marginals = tf.nn.softmax(t_pout,axis=0)\n",
    "\n",
    "        if(debug):\n",
    "            print(\"marginals\",marginals)\n",
    "        predict = tf.argmax(marginals,axis=0)\n",
    "\n",
    "\n",
    "        train_step = tf.train.AdamOptimizer(lr).minimize(loss)#, var_list=[thetas, alphas]) \n",
    "\n",
    "\n",
    "        init_g = tf.global_variables_initializer()\n",
    "        init_l = tf.local_variables_initializer()\n",
    "        \n",
    "        dev_high = np.zeros(3)\n",
    "        test_high = np.zeros(3)\n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init_g)\n",
    "            sess.run(init_l)\n",
    "\n",
    "            flg = False\n",
    "            # Initialize an iterator over the training dataset.\n",
    "            for en in range(ep):\n",
    "                sess.run(train_init_op)\n",
    "#                 temprun = sess.run(next_element2)\n",
    "#                 print('len next ',len(temprun), len(temprun[0]))\n",
    "#                 print(temprun[0])\n",
    "                tl = 0\n",
    "                pll = 0\n",
    "                unl = 0\n",
    "                try:\n",
    "                    it = 0\n",
    "                    \n",
    "                    grad_sum = 0.0\n",
    "                    while True:\n",
    "#                         print(en, it)\n",
    "\n",
    "                        _,ls,ploss,xpp,t = sess.run([train_step,loss,prec_loss,xtra,thetas])\n",
    "                        print(\"Ep \", en, \" it \", it, \" thetas= \", t)\n",
    "                        if(penalty == 42):\n",
    "                            un_loss = sess.run(temp_uloss)\n",
    "\n",
    "                        tl = tl + ls\n",
    "                        pll = pll + ploss\n",
    "                        it = it + 1\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    pass\n",
    "                opstring = \"\"\n",
    "                print(en,\"loss\",tl)\n",
    "                opstring = opstring + str(en) + \",\" + str(tl) + \",\"\n",
    "#                 print('temp__', temp__)\n",
    "                print(\"un loss\", unl)\n",
    "                print(\"ploss\", pll)\n",
    "                print(\"ratio\", unl/pll)\n",
    "                print(\"extra\", xpp)\n",
    "#                 dloss = sess.run(dev_loss)\n",
    "#                 print(\"dloss\", dloss)\n",
    "#                 print(\"expected ratio\", xpp/(xpp+1))\n",
    "                print(\"dev set\")\n",
    "                sess.run(dev_init_op)\n",
    "##                 sm,a,t,m,pl = sess.run([summary_merged,alphas,thetas,marginals,predict])\n",
    "#                 a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "                a,t,m,pl,ddloss = sess.run([alphas,thetas,marginals,predict,loss])\n",
    "##                 test_writer.add_summary(sm, en)\n",
    "                print('init dev loss', ddloss)\n",
    "                opstring = opstring + str(ddloss) + \",\"\n",
    "                print(\"Iter\", en, \" alphas=\", a, \" thetas=\", a)\n",
    "            \n",
    "                unique, counts = np.unique(pl, return_counts=True)\n",
    "                print(dict(zip(unique, counts)))\n",
    "                print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "                dev_results = precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\")\n",
    "                print('devresults ', dev_results)\n",
    "                roc_dev = roc_auc_score(np.array(gold_labels_dev), np.array(m[1,:,0]))\n",
    "                print('roc score', roc_dev)\n",
    "                opstring = str(en) + ',' #do not print losses\n",
    "                opstring = opstring + str(dev_results[2]) + \",\" + str(roc_dev) + \",\"\n",
    "#                 if(dev_results[2]>dev_high[2]):\n",
    "                dev_high = np.array(dev_results)\n",
    "#                 dev_high = max(dev_high, dev_results[2])\n",
    "                print()\n",
    "                print(\"test set\")\n",
    "                sess.run(test_init_op)\n",
    "                a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "                unique, counts = np.unique(pl, return_counts=True)\n",
    "                print(dict(zip(unique, counts)))\n",
    "                print(\"acc\",accuracy_score(gold_labels_test,pl))\n",
    "                test_results = precision_recall_fscore_support(np.array(gold_labels_test),np.array(pl),average=\"binary\")\n",
    "                print('testresults ', test_results)\n",
    "                roc_test = roc_auc_score(np.array(gold_labels_test), np.array(m[1,:,0]))\n",
    "                print('roc score', roc_test)\n",
    "                opstring = opstring + str(test_results[2]) + \",\" + str(roc_test)\n",
    "#                 if(test_results[2]>test_high[2]):\n",
    "                test_high = np.array(test_results)\n",
    "                print()\n",
    "        \n",
    "                #sys.stdout = stdout_saved\n",
    "                # print(opstring)\n",
    "# #                 newptheta = sess.run(marginals)\n",
    "#                 print(\"new z \", ztemp)\n",
    "#                 print(\"new z shape \", np.array(ztemp).shape)\n",
    "#                 print('shape ',np.array(newptheta).shape)\n",
    "               # sys.stdout = open(\"trash\", \"w\")\n",
    "                \n",
    "\n",
    "    return pl, m, dev_high, opstring#test_high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ 107\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "dim 1 not in the interval [-1, 0]. for 'map/while/ExpandDims_1' (op: 'ExpandDims') with input shapes: [], [] and with computed input tensors: input[1] = <1>.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/home/sunita/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1575\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1576\u001b[0;31m     \u001b[0mc_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_FinishOperation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1577\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: dim 1 not in the interval [-1, 0]. for 'map/while/ExpandDims_1' (op: 'ExpandDims') with input shapes: [], [] and with computed input tensors: input[1] = <1>.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-0bbcbacae1b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m#        sys.stdout = stdout_saved\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         _, _, _, ops = train2(1/len(train_L_S),2,batch_size = b, th = tf.truncated_normal_initializer(1,0.1,seed),                                    af = tf.truncated_normal_initializer(0,0.1,seed),                                    user_a = ua,                                    LF_acc = np.repeat(0.4, len(LF_l)) ,pcl=np.array([-1,1],dtype=np.float64),                                    norm=True,smooth=True,penalty=pen_i, Gamma=gg, debug=False,                                    a_pr = np.repeat(0.65, len(LF_l)),                                        n_pr = np.repeat(100, len(LF_l)),                                     isdiscrete = dscr, alpha_max=np.repeat(0.9, len(LF_l)),\n\u001b[0;32m---> 26\u001b[0;31m                              r_pr = np.repeat(0.750, len(LF_l)))\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0morig_stdout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# get_LF_acc(dev_L_S,gold_labels_dev)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-9ef0797c600a>\u001b[0m in \u001b[0;36mtrain2\u001b[0;34m(lr, ep, th, af, batch_size, LF_acc, LF_rec, pcl, norm, smooth, penalty, p3k, alp, Gamma, debug, r1, r2, a_pr, n_pr, user_a, isdiscrete, alpha_max, r_pr)\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpenalty\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0mnumYs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m             \u001b[0mloss_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mper_lf_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarginals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mper_lf_recall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpots\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_only\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumYs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthetas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malphas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0misdiscrete\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpenalty\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0malpha_max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m             \u001b[0mloss_precision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_only\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecision_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mper_lf_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpenalty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mr_t\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vista/Users/admin/akash/code/ContinuousLFs/model_only.py\u001b[0m in \u001b[0;36mmodel\u001b[0;34m(numYs, k, l, s, thetas, alpha_vars, isdiscrete, user_a, penalty, alpha_max_arg, s_thresholds_precision)\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsgActive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_thresholds_precisions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m   \u001b[0mlogz_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogmsg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msbins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m   \u001b[0mlogZ_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_logsumexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogz_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sunita/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/functional_ops.py\u001b[0m in \u001b[0;36mmap_fn\u001b[0;34m(fn, elems, dtype, parallel_iterations, back_prop, swap_memory, infer_shape, name)\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[0mback_prop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mback_prop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m         \u001b[0mswap_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswap_memory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m         maximum_iterations=n)\n\u001b[0m\u001b[1;32m    460\u001b[0m     \u001b[0mresults_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mr_a\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sunita/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mwhile_loop\u001b[0;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations, return_same_structure)\u001b[0m\n\u001b[1;32m   3230\u001b[0m       \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_to_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWHILE_CONTEXT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloop_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3231\u001b[0m     result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants,\n\u001b[0;32m-> 3232\u001b[0;31m                                     return_same_structure)\n\u001b[0m\u001b[1;32m   3233\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmaximum_iterations\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3234\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sunita/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mBuildLoop\u001b[0;34m(self, pred, body, loop_vars, shape_invariants, return_same_structure)\u001b[0m\n\u001b[1;32m   2950\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mutation_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2951\u001b[0m         original_body_result, exit_vars = self._BuildLoop(\n\u001b[0;32m-> 2952\u001b[0;31m             pred, body, original_loop_vars, loop_vars, shape_invariants)\n\u001b[0m\u001b[1;32m   2953\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2954\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sunita/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36m_BuildLoop\u001b[0;34m(self, pred, body, original_loop_vars, loop_vars, shape_invariants)\u001b[0m\n\u001b[1;32m   2885\u001b[0m         flat_sequence=vars_for_body_with_tensor_arrays)\n\u001b[1;32m   2886\u001b[0m     \u001b[0mpre_summaries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SUMMARY_COLLECTION\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2887\u001b[0;31m     \u001b[0mbody_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpacked_vars_for_body\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2888\u001b[0m     \u001b[0mpost_summaries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SUMMARY_COLLECTION\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2889\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sunita/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(i, lv)\u001b[0m\n\u001b[1;32m   3199\u001b[0m         cond = lambda i, lv: (  # pylint: disable=g-long-lambda\n\u001b[1;32m   3200\u001b[0m             math_ops.logical_and(i < maximum_iterations, orig_cond(*lv)))\n\u001b[0;32m-> 3201\u001b[0;31m         \u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlv\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3203\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sunita/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/functional_ops.py\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(i, tas)\u001b[0m\n\u001b[1;32m    446\u001b[0m       \"\"\"\n\u001b[1;32m    447\u001b[0m       \u001b[0mpacked_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_pack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0melem_ta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0melem_ta\u001b[0m \u001b[0;32min\u001b[0m \u001b[0melems_ta\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m       \u001b[0mpacked_fn_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpacked_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m       \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_same_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0melems\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpacked_fn_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m       \u001b[0mflat_fn_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_flatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpacked_fn_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vista/Users/admin/akash/code/ContinuousLFs/model_only.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsgActive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_thresholds_precisions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m   \u001b[0mlogz_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogmsg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msbins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m   \u001b[0mlogZ_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_logsumexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogz_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vista/Users/admin/akash/code/ContinuousLFs/model_only.py\u001b[0m in \u001b[0;36mlogmsg\u001b[0;34m(s, y, l)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mlogmsg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m       return tf.nn.softplus((1-is_discrete) * (tf.reduce_logsumexp(pot(s, y,l), axis=0)+tf.log(sbin_widths))\n\u001b[0m\u001b[1;32m    155\u001b[0m                               +  (is_discrete *  dis_pot(y, l)))\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vista/Users/admin/akash/code/ContinuousLFs/model_only.py\u001b[0m in \u001b[0;36mpot\u001b[0;34m(s, y, l)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mpot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mis_discrete\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mcont_pot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mis_discrete\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdis_pot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vista/Users/admin/akash/code/ContinuousLFs/model_only.py\u001b[0m in \u001b[0;36mdis_pot\u001b[0;34m(y, l)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mdis_pot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mthetas\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ml\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnumYs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mpotType\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m7\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mbatch_gather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthetas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscrete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mequal_sign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mpot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vista/Users/admin/akash/code/ContinuousLFs/model_only.py\u001b[0m in \u001b[0;36mbatch_gather\u001b[0;34m(x, idx)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;31m# idx = [numRows]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m# collects the column of x for each row as specified in the idx.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mcat_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather_nd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcat_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sunita/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    452\u001b[0m                 \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m                 instructions)\n\u001b[0;32m--> 454\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m     return tf_decorator.make_decorator(func, new_func, 'deprecated',\n\u001b[1;32m    456\u001b[0m                                        _add_deprecated_arg_notice_to_docstring(\n",
      "\u001b[0;32m/home/sunita/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mexpand_dims\u001b[0;34m(input, axis, name, dim)\u001b[0m\n\u001b[1;32m    134\u001b[0m   \"\"\"\n\u001b[1;32m    135\u001b[0m   \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeprecation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeprecated_argument_lookup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"axis\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dim\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sunita/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mexpand_dims\u001b[0;34m(input, axis, name)\u001b[0m\n\u001b[1;32m   2018\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_ctx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eager_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2019\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[0;32m-> 2020\u001b[0;31m         \"ExpandDims\", input=input, dim=axis, name=name)\n\u001b[0m\u001b[1;32m   2021\u001b[0m     \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2022\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sunita/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    785\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    786\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    788\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sunita/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    452\u001b[0m                 \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m                 instructions)\n\u001b[0;32m--> 454\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m     return tf_decorator.make_decorator(func, new_func, 'deprecated',\n\u001b[1;32m    456\u001b[0m                                        _add_deprecated_arg_notice_to_docstring(\n",
      "\u001b[0;32m/home/sunita/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3153\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3154\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3155\u001b[0;31m           op_def=op_def)\n\u001b[0m\u001b[1;32m   3156\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_op_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3157\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sunita/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[1;32m   1729\u001b[0m           op_def, inputs, node_def.attr)\n\u001b[1;32m   1730\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\n\u001b[0;32m-> 1731\u001b[0;31m                                 control_input_ops)\n\u001b[0m\u001b[1;32m   1732\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1733\u001b[0m     \u001b[0;31m# Initialize self._outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sunita/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1577\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1578\u001b[0m     \u001b[0;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1579\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1581\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mc_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: dim 1 not in the interval [-1, 0]. for 'map/while/ExpandDims_1' (op: 'ExpandDims') with input shapes: [], [] and with computed input tensors: input[1] = <1>."
     ]
    }
   ],
   "source": [
    "#added 2\n",
    "# import sys\n",
    "# sys.path.append(\"../../../models\")\n",
    "# from new_01 import train2\n",
    "\n",
    "a = 0.60\n",
    "ua = np.repeat(a, len(LF_l))\n",
    "dscr = np.repeat(data_discrete, len(LF_l))\n",
    "\n",
    "# import sys\n",
    "# stdout_saved = sys.stdout\n",
    "stdout_saved = orig_stdout\n",
    "# for rx in [1, len(LF_l), len(train_L_S)]:\n",
    "# for i in np.linspace(1,1,1):\n",
    "b = 32\n",
    "for pen_i in [107]:#[0, 4, 42]:\n",
    "# for b in [32]:#[32,64,128,512,1024,2048,4096]:\n",
    "    devmax = np.zeros(3)\n",
    "    testmax = np.zeros(3)\n",
    "    for gg in range(1):#[1]:#np.linspace(0,100,101):#, 10, 30, 50, 100]:\n",
    "#         print(\"batch-size:\",b, \"gamma:\",gg)\n",
    "        print('============', pen_i)\n",
    "        sys.stdout = open('trash', 'w')\n",
    "#        sys.stdout = stdout_saved\n",
    "        _, _, _, ops = train2(1/len(train_L_S),2,batch_size = b, th = tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "                                    af = tf.truncated_normal_initializer(0,0.1,seed),\\\n",
    "                                    user_a = ua,\\\n",
    "                                    LF_acc = np.repeat(0.4, len(LF_l)) ,pcl=np.array([-1,1],dtype=np.float64),\\\n",
    "                                    norm=True,smooth=True,penalty=pen_i, Gamma=gg, debug=False,\\\n",
    "                                    a_pr = np.repeat(0.65, len(LF_l)), \\\n",
    "                                       n_pr = np.repeat(100, len(LF_l)),\\\n",
    "                                     isdiscrete = dscr, alpha_max=np.repeat(0.9, len(LF_l)),\n",
    "                             r_pr = np.repeat(0.750, len(LF_l)))\n",
    "        sys.stdout = orig_stdout\n",
    "# get_LF_acc(dev_L_S,gold_labels_dev)\n",
    "#                 if(devmax_t[2]>devmax[2]):\n",
    "#         devmax = np.array(devmax_t)\n",
    "#                 if(testmax_t[2]>testmax[2]):\n",
    "#         testmax = np.array(testmax_t)\n",
    "#         r42d.append(devmax)\n",
    "#         r42t.append(testmax)\n",
    "#         print(\"\\n\\nfinal_result_line: \",\"thetas:\",0,0.1,\"batch size\",b,\"penalty\",pen_i,)\n",
    "##        sys.stdout = stdout_saved\n",
    "#         print(\"\\n\\nfinal_result_line: \",\"thetas:\",0,0.1,\"batch size\",b,\"penalty\",pen_i,)\n",
    "# sys.stdout = open('opfile2', 'a')\n",
    "# print('rate = 1/train_len')      \n",
    "# sys.stdout = stdout_saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
