{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#call this only once for a kernel startup\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import model_only\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "from numpy import random as rand\n",
    "import math\n",
    "\n",
    "# BATCH_SIZE = 32\n",
    "seed = 12\n",
    "\n",
    "\n",
    "orig_stdout = sys.stdout\n",
    "\n",
    "data_discrete = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gauss_sep = 0.1\n",
    "gauss_var = 0.4\n",
    "discrete_threshold = 0.85\n",
    "smooth_threshold = 0.7\n",
    "threshold = [0.7, 0.85]\n",
    "explambda=0.8\n",
    "\n",
    "def gauss_plus():\n",
    "    return np.clip(np.random.normal(0.5 + gauss_sep, gauss_var), 0, 1)\n",
    "    print(gauss_plus(5))\n",
    "\n",
    "def gauss_minus():\n",
    "    return np.clip(np.random.normal(0.5 - gauss_sep, gauss_var), 0, 1)\n",
    "    print(gauss_minus(5))\n",
    "    \n",
    "def exp_plus():\n",
    "    return np.clip(np.random.exponential(explambda), 0, 1)\n",
    "\n",
    "def exp_minus():\n",
    "    return 1 - exp_plus()\n",
    "    \n",
    "def gauss_random_pos():\n",
    "    prob = 0.5 + epsilon\n",
    "    sample = np.random.rand()\n",
    "    if(sample<prob):\n",
    "        return gauss_plus()\n",
    "    return gauss_minus()\n",
    "\n",
    "def gauss_random_neg():\n",
    "    prob = 0.5 - epsilon\n",
    "    sample = np.random.rand()\n",
    "    if(sample<prob):\n",
    "        return gauss_plus()\n",
    "    return gauss_minus()\n",
    "\n",
    "#discrete\n",
    "def generateAdversarial(k, is_discrete):\n",
    "    f = np.empty([2, instances])\n",
    "#     if(is_discrete):\n",
    "    if k==-1:\n",
    "        for i,x in enumerate(labels):\n",
    "#             s = gauss[int(x)]()[0]\n",
    "            l = 0\n",
    "            if(x):\n",
    "                s = gauss_plus()\n",
    "                if(s < 1-threshold[data_discrete]):\n",
    "                    l = -1\n",
    "            else:\n",
    "                s = gauss_minus()\n",
    "                if(s > threshold[data_discrete]):\n",
    "                    l = -1\n",
    "            f[0][i] = l\n",
    "            f[1][i] = s\n",
    "    else:\n",
    "        for i,x in enumerate(labels):\n",
    "#             s = gauss[int(x)]()[0]\n",
    "            l = 0\n",
    "            if(x):\n",
    "                s = gauss_plus()\n",
    "                if(s > threshold[data_discrete]):\n",
    "                    l = 1\n",
    "            else:\n",
    "                s = gauss_minus()\n",
    "                if(s < 1-threshold[data_discrete]):\n",
    "                    l = 1\n",
    "            f[0][i] = l\n",
    "            f[1][i] = s\n",
    "    return f\n",
    "\n",
    "#discrete\n",
    "def generate(k, is_discrete):\n",
    "    f = np.empty([2, instances])\n",
    "    for i,x in enumerate(labels):\n",
    "        s = gauss_plus() if (x) else gauss_minus()\n",
    "        f[0][i] = -1 if (s < 1-threshold[data_discrete] and k==-1) else 0\n",
    "        f[0][i] = 1 if (s > threshold[data_discrete] and k==1) else 0\n",
    "        f[1][i] = s\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62.264283176 42.073067056\n"
     ]
    }
   ],
   "source": [
    "pos = [exp_plus() for i in range(100)]\n",
    "negs =  [exp_minus() for i in range(100)]\n",
    "print (np.sum(pos), np.sum(negs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 520\n",
      "(1000, 2, 3)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "seed = 12\n",
    "rand.seed(12)\n",
    "\n",
    "instances = 1000\n",
    "epsilon = 0.02 # 0.50 + epsilon = correlation\n",
    "ratio = 0.5 # ones:total\n",
    "noOfFunctions = 2\n",
    "\n",
    "correlatedInstances = math.ceil((0.5+epsilon)*instances)\n",
    "indices = np.arange(0,instances,1)\n",
    "\n",
    "numOnes = math.ceil(ratio*instances)\n",
    "numZeros = instances - numOnes\n",
    "onesArray = np.ones(numOnes)#, dtype=int)\n",
    "zerosArray = np.zeros(numZeros)#, dtype=int)\n",
    "labels = np.concatenate((onesArray, zerosArray))\n",
    "rand.shuffle(labels) #shuffle datapoints\n",
    "\n",
    "print(np.count_nonzero(labels), correlatedInstances)\n",
    "\n",
    "functions=[]\n",
    "\n",
    "# print(\"before\")\n",
    "# print(labels)\n",
    "\n",
    "\n",
    "for i in range(math.ceil(noOfFunctions/2)):\n",
    "    index_i = rand.choice(indices, size = correlatedInstances, replace=False)\n",
    "#         f = [1-labels, np.random.uniform(0,0.75, len(labels))]\n",
    "    f = generate(1, data_discrete)\n",
    "#         print(f)\n",
    "    functions.append(f)\n",
    "\n",
    "for i in range(math.floor(noOfFunctions/2)):\n",
    "    index_i = rand.choice(indices, size = correlatedInstances, replace=False)\n",
    "#         f = [-1*labels, np.random.uniform(0, 0.75, len(labels))]\n",
    "    f = generate(-1, data_discrete)\n",
    "#         print(f)\n",
    "    functions.append(f)\n",
    "    \n",
    "functions.append([labels, labels])\n",
    "functions = np.transpose(np.array(functions))\n",
    "print(functions.shape)\n",
    "y=functions\n",
    "print(type(y[0]))\n",
    "print(type(y))\n",
    "# print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          0.        ]\n",
      " [ 0.24047209  0.28596173  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(y[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 500 1000\n",
      "400 100 400 100\n",
      "(800, 2, 3)\n",
      "(200, 2, 3)\n"
     ]
    }
   ],
   "source": [
    "last = len(y[0][0])-1\n",
    "ones = [p for p in y if p[0][last]==1]\n",
    "zeros = [p for p in y if p[0][last]==0]\n",
    "\n",
    "print(len(zeros),len(ones),len(y))\n",
    "\n",
    "train_set=[]\n",
    "test_set=[]\n",
    "\n",
    "split=0.8\n",
    "\n",
    "# split into train and test\n",
    "# same distribution of 1s\n",
    "\n",
    "train_split=int(split*len(y))\n",
    "test_split=len(y)-train_split\n",
    "\n",
    "train_ones = int(split*len(ones))\n",
    "test_ones = len(ones)-train_ones\n",
    "train_zeros = train_split-train_ones\n",
    "test_zeros = test_split-test_ones\n",
    "\n",
    "print(train_ones, test_ones, train_zeros, test_zeros)\n",
    "j=0\n",
    "k=0\n",
    "for i in range(train_split):\n",
    "    if(j<i*train_ones/train_split):\n",
    "        train_set.append(ones[j])\n",
    "        j = j +1\n",
    "    else:\n",
    "        train_set.append(zeros[k])\n",
    "        k = k +1\n",
    "train_set = np.array(train_set)\n",
    "print(train_set.shape)\n",
    "# print(np.sum(train_set[:,last]), len(train_set))\n",
    "\n",
    "j2=0\n",
    "k2=0\n",
    "for i in range(test_split):\n",
    "    if(j2<i*test_ones/test_split):\n",
    "        test_set.append(ones[j])\n",
    "        j = j +1\n",
    "        j2= j2+1\n",
    "    else:\n",
    "        test_set.append(zeros[k])\n",
    "        k = k +1\n",
    "        k2= k2+1\n",
    "test_set = np.array(test_set)\n",
    "print(test_set.shape)\n",
    "rand.shuffle(train_set)\n",
    "rand.shuffle(test_set)\n",
    "# print(np.sum(test_set[:,last]), len(test_set))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save(\"train_synthetic_smooth\",train_set)\n",
    "np.save(\"test_synthetic_smooth\", test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 2, 3)\n"
     ]
    }
   ],
   "source": [
    "print(test_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200,)\n",
      "100\n",
      "[ 0.  0.  0.  1.  0.  0.  0.  1.  0.  1.  1.  1.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.]\n",
      "2 [ 1. -1.]\n"
     ]
    }
   ],
   "source": [
    "from numpy import genfromtxt\n",
    "\n",
    "temp = np.load(\"test_synthetic_smooth.npy\")\n",
    "true_labels = temp[:,0,last]\n",
    "gold_labels_dev = true_labels\n",
    "gold_labels_test = true_labels\n",
    "print(true_labels.shape)\n",
    "print(np.count_nonzero(true_labels))\n",
    "print(true_labels[:20])\n",
    "\n",
    "# mapping class labels to labeling functions\n",
    "# can consider distance functions as giving a positively correlated score to the negative class\n",
    "LF_l = np.concatenate((np.ones(math.ceil(noOfFunctions/2)), -1+np.zeros(math.floor(noOfFunctions/2))))\n",
    "print(len(LF_l), LF_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def draw2DArray(a):\n",
    "    fig = plt.figure(figsize=(6, 3.2))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_title('colorMap')\n",
    "    plt.imshow(np.array(a))\n",
    "    ax.set_aspect('equal')\n",
    "    cax = fig.add_axes([0.12, 0.1, 0.78, 0.8])\n",
    "    cax.get_xaxis().set_visible(False)\n",
    "    cax.get_yaxis().set_visible(False)\n",
    "    cax.patch.set_alpha(0)\n",
    "    cax.set_frame_on(False)\n",
    "    plt.colorbar(orientation='vertical')\n",
    "    plt.show()\n",
    "    \n",
    "      \n",
    "def report2dict(cr):\n",
    "    # Parse rows\n",
    "    tmp = list()\n",
    "    for row in cr.split(\"\\n\"):\n",
    "        parsed_row = [x for x in row.split(\"  \") if len(x) > 0]\n",
    "        if len(parsed_row) > 0:\n",
    "            tmp.append(parsed_row)\n",
    "    \n",
    "    # Store in dictionary\n",
    "    measures = tmp[0]\n",
    "\n",
    "    D_class_data = defaultdict(dict)\n",
    "    for row in tmp[1:]:\n",
    "        class_label = row[0]\n",
    "        for j, m in enumerate(measures):\n",
    "            D_class_data[class_label][m.strip()] = float(row[j + 1].strip())\n",
    "    return pd.DataFrame(D_class_data).T\n",
    "\n",
    "def predictAndPrint(pl):\n",
    "    print(\"acc\",accuracy_score(true_labels,pl))\n",
    "#     print(precision_recall_fscore_support(true_labels,pl,average='macro'))\n",
    "    print(confusion_matrix(true_labels,pl))\n",
    "#     draw2DArray(confusion_matrix(gold_labels_dev,pl))\n",
    "    return report2dict(classification_report(true_labels, pl))# target_names=class_names))\n",
    "    \n",
    "\n",
    "\n",
    "def drawPRcurve(y_test,y_score,it_no):\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    splt = fig.add_subplot(111)\n",
    "    \n",
    "\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_score,pos_label=1)\n",
    "\n",
    "    splt.step(recall, precision, color='b', alpha=0.2,\n",
    "             where='post')\n",
    "    splt.fill_between(recall, precision, step='post', alpha=0.2,\n",
    "                     color='b')\n",
    "    average_precision = average_precision_score(y_test, y_score)\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlim([0.0, 1.05])\n",
    "    plt.title('{0:d} Precision-Recall curve: AP={1:0.2f}'.format(it_no,\n",
    "              average_precision))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 2, 2)\n"
     ]
    }
   ],
   "source": [
    "print(train_set[:,:,:-1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape (200,)\n",
      "(200, 2, 2) (800, 2, 2)\n",
      "2\n",
      "2\n",
      "[[ 0.          0.        ]\n",
      " [ 0.06344483  0.0208054 ]]\n"
     ]
    }
   ],
   "source": [
    "#load dataset\n",
    "test_L_S = np.load(\"test_synthetic_smooth.npy\")\n",
    "train_L_S = np.load(\"train_synthetic_smooth.npy\")\n",
    "dev_L_S = test_L_S\n",
    "\n",
    "gold_labels_test = test_L_S[:,0,-1]\n",
    "gold_labels_dev = gold_labels_test\n",
    "print('shape', gold_labels_test.shape)\n",
    "\n",
    "test_L_S = np.delete(test_L_S, -1, 2)#test_L_S[:][:][:-1]\n",
    "train_L_S = np.delete(train_L_S, -1, 2)#train_L_S[:][:][:-1]\n",
    "dev_L_S = np.delete(dev_L_S, -1, 2)#dev_L_S[:][:][:-1]\n",
    "\n",
    "print(test_L_S.shape,train_L_S.shape)\n",
    "NoOfClasses=2\n",
    "print(NoOfClasses)\n",
    "NoOfLFs = test_L_S.shape[2]\n",
    "print(NoOfLFs)\n",
    "\n",
    "# test_L_S[test_L_S[:, 0, 9] == LF_l[9],0,9]\n",
    "# LF_names= [lf.__name__ for lf in LFs]\n",
    "print(train_L_S[0,:,:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 0.02 2\n"
     ]
    }
   ],
   "source": [
    "sys.stdout = orig_stdout\n",
    "print('test', epsilon, noOfFunctions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#added new model\n",
    "def train2(lr,ep,th,af,batch_size=32,LF_acc=None,LF_rec=None,pcl=np.array([-1,1],dtype=np.float64),norm=True,\\\n",
    "          smooth=True,penalty=0,p3k=3,alp=1,Gamma=1.0,debug=True, \\\n",
    "           r1=1.0, r2=1.0, a_pr=None, n_pr=None, user_a = None, isdiscrete = None, alpha_max = None, r_pr=None):\n",
    "    \n",
    "    ## lr : learning rate\n",
    "    ## ep : no of epochs\n",
    "    ## th : thetas initializer\n",
    "    ## af : alphas initializer\n",
    "    ## penalty : {1,2,3} use one of the three penalties, 0: no-penalty\n",
    "    ## p3k : parameter for penalty-3 \n",
    "    ## smooth : flag if smooth lfs are used \n",
    "    ## make sure smooth/discrete LF data is loaded into train_L_S and test_L_S\n",
    "    ## pcl : all possible class labels  = [-1,1] for binary, \n",
    "    ##       np.arange(0,NoOfClasses) for multiclass\n",
    "    ## alp : alpha parameter (to set a max value for alpha)\n",
    "    ## norm : use normalization or not\n",
    "    ## Gamma : penalty tuning parameter\n",
    "    \n",
    "    BATCH_SIZE = batch_size\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    seed = 12\n",
    "    \n",
    "    LF_k = []\n",
    "    if(len(pcl)==2):\n",
    "        for i in range(len(LF_l)):\n",
    "            LF_k.append(int((LF_l[i]+1)/2))\n",
    "    with tf.Graph().as_default():\n",
    "        xtra = tf.Variable(0, trainable=False)\n",
    "        gammaR = tf.constant(0, dtype=tf.float64)\n",
    "\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices(train_L_S).batch(BATCH_SIZE)\n",
    "        dev_dataset = tf.data.Dataset.from_tensor_slices(dev_L_S).batch(len(dev_L_S))\n",
    "        test_dataset = tf.data.Dataset.from_tensor_slices(test_L_S).batch(len(test_L_S))\n",
    "\n",
    "        iterator = tf.data.Iterator.from_structure(train_dataset.output_types, train_dataset.output_shapes)\n",
    "        next_element = iterator.get_next()\n",
    "\n",
    "        train_init_op = iterator.make_initializer(train_dataset)\n",
    "        dev_init_op = iterator.make_initializer(dev_dataset)\n",
    "        test_init_op = iterator.make_initializer(test_dataset)\n",
    "\n",
    "        next_element = iterator.get_next()\n",
    "\n",
    "        alphas = tf.get_variable('alphas', [NoOfLFs], initializer=af, dtype=tf.float64)\n",
    "        if(penalty in [103, 113]):\n",
    "            alphas = tf.stop_gradient(alphas*0)\n",
    "            \n",
    "        thetas = tf.get_variable('thetas', [1,NoOfLFs], initializer=th, dtype=tf.float64)\n",
    "        \n",
    "        LF_label = tf.convert_to_tensor(LF_k, dtype=tf.int32)\n",
    "        k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
    "        g = tf.convert_to_tensor(Gamma, dtype=tf.float64)\n",
    "        \n",
    "        if(penalty in [4,5,7,8,9,10,11, 44, 42, 41]):\n",
    "            LF_a = tf.convert_to_tensor(LF_acc, dtype=tf.float64)\n",
    "        \n",
    "        if(penalty in [6,7,11,12,15]):\n",
    "            LF_r = tf.convert_to_tensor(LF_rec, dtype=tf.float64)\n",
    "        \n",
    "        l,s =  tf.unstack(next_element,axis=1)\n",
    "\n",
    "        if(smooth and penalty < 100):\n",
    "            s_ = tf.maximum(tf.subtract(s,tf.minimum(alphas,alp)), 0)\n",
    "            if(debug):\n",
    "                print(\"s_\",s_)\n",
    "\n",
    "        def iskequalsy(v,s):\n",
    "            out = tf.where(tf.equal(v,s),tf.ones_like(v),-tf.ones_like(v))\n",
    "            if(debug):\n",
    "                print(\"out\",out)\n",
    "            return out\n",
    "#MAP\n",
    "        if(penalty < 100):\n",
    "            if(smooth):\n",
    "                pout = tf.map_fn(lambda c: l*c*s_ ,pcl,name=\"pout\")\n",
    "            else:\n",
    "                pout = tf.map_fn(lambda c: l*c ,pcl,name=\"pout\")\n",
    "\n",
    "            t =  tf.squeeze(thetas)                \n",
    "                \n",
    "            def ints(y): # called for y=1 and y=-1\n",
    "                ky = iskequalsy(k,y)  # ky = 1 if k==y else -1\n",
    "                if(debug):\n",
    "                    print(\"ky\",ky)\n",
    "                out1 = alphas+((tf.exp((t*ky*(1-alphas)))-1)/(t*ky))\n",
    "                if(debug):\n",
    "                    print(\"intsy\",out1)\n",
    "                return out1                \n",
    "                \n",
    "            if(smooth):\n",
    "                #smooth normalizer\n",
    "                zy = tf.map_fn(lambda y: tf.reduce_prod(1+ints(y),axis=0),\\\n",
    "                               pcl,name=\"zy\")\n",
    "            else:\n",
    "                #discrete normalizer\n",
    "                zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t*iskequalsy(k,y)),axis=0),\\\n",
    "                               pcl,name=\"zy\")            \n",
    "#new model\n",
    "\n",
    "        if(penalty > 200):\n",
    "\n",
    "#             z_y_new = [tf.reduce_prod(1 + 1/(-1*k*thetas) * (tf.exp(-1*k*thetas) - tf.exp(-1*k*thetas*user_alphas)) ), \\\n",
    "#                    tf.reduce_prod(1 + 1/(1*k*thetas) * (tf.exp(1*k*thetas) - tf.exp(1*k*thetas*user_alphas)) ) ]\n",
    "            numbins = tf.constant(10, dtype=tf.float64)\n",
    "            #TODO: 2 binned integration over the s values to allow stable gradients.\n",
    "            sbin_widths = (1-user_alphas)/numbins\n",
    "            sbins = tf.einsum('i,j->ij', sbin_widths, tf.cast(tf.range(0,numbins+1), dtype=tf.float64))+tf.expand_dims(user_alphas,1)\n",
    "            sbins = tf.transpose(sbins)\n",
    "            \n",
    "            def pot(s):\n",
    "                if(penalty%10 == 1):\n",
    "                    return thetas*s-alphas\n",
    "                elif(penalty%10 == 2):\n",
    "                    return thetas*tf.clip_by_value(s-alphas,0,1)\n",
    "                elif(penalty%10 == 3):\n",
    "                    return thetas*s\n",
    "                return 0\n",
    "                                       \n",
    "            # over the y-s it is okay to use a map function, but not over the LFs.\n",
    "            z_y_new = tf.map_fn(lambda y: tf.reduce_prod(1 + tf.reduce_sum(tf.exp(-y*k*pot(sbins)), axis=0)), tf.constant([1,-1], dtype=tf.float64))\n",
    "#             z_y_new = tf.reduce_sum(tf.exp(-1*k*thetas*sbins), axis=0)*sbin_widths/(1-user_alphas)\n",
    "# *sbin_widths\n",
    "\n",
    "            Z_new = tf.reduce_sum(z_y_new)\n",
    "            print(\"new z \", Z_new)\n",
    "\n",
    "#p theta without exp [y x i]\n",
    "            # snap s_ to the nearest bin. \n",
    "            s_ = tf.round((s_-user_alphas)*numbins/(1-user_alphas))*sbin_widths + user_alphas\n",
    "            logp_theta_new_t = tf.map_fn(lambda y:  tf.reduce_sum(-y*l*pot(s_), axis=1), tf.constant([1,-1], dtype=tf.float64))\n",
    "\n",
    "\n",
    "            loss_new_t = tf.reduce_logsumexp(logp_theta_new_t, axis=0) - tf.log(Z_new)\n",
    "            loss_new = tf.negative(tf.reduce_sum(loss_new_t))\n",
    "        \n",
    "#marginals as softmax\n",
    "\n",
    "            marginals_new = tf.expand_dims(tf.nn.softmax(logp_theta_new_t, axis=0), 2)\n",
    "\n",
    "        if(penalty < 100):\n",
    "            \n",
    "            if(debug):\n",
    "                print(\"pout\",pout)    \n",
    "    #MAP\n",
    "            t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout,\\\n",
    "                               name=\"t_pout\")\n",
    "\n",
    "            if(debug):\n",
    "                print(\"t_pout\",t_pout)\n",
    "\n",
    "        t =  tf.squeeze(thetas)\n",
    "        if(debug):\n",
    "            print(\"t\",t)\n",
    "\n",
    "        if(penalty < 100):\n",
    "            marginals = tf.nn.softmax(t_pout,axis=0)\n",
    "#         else:\n",
    "#             marginals = marginals_new\n",
    "        \n",
    "            if(debug):\n",
    "                print(\"zy\",zy)\n",
    "            logz = tf.log(tf.reduce_sum(zy,axis=0),name=\"logz\")\n",
    "            if(debug):\n",
    "                print(\"logz\",logz)\n",
    "    ##         tf.summary.scalar('logz', logz)\n",
    "            lsp = tf.reduce_logsumexp(t_pout,axis=0)\n",
    "            if(debug):\n",
    "                print(\"lsp\",lsp)\n",
    "##         tf.summary.scalar('lsp', tf.reduce_sum(lsp))\n",
    "        \n",
    "    \n",
    "        if(penalty == 20 or penalty > 100):\n",
    "            a_t = tf.convert_to_tensor(a_pr, dtype=tf.float64)\n",
    "            n_t = tf.convert_to_tensor(n_pr, dtype=tf.float64)\n",
    "            r_t = tf.convert_to_tensor(r_pr, dtype=tf.float64)\n",
    "        def get_loss_20():\n",
    "#NEW MODEL\n",
    "\n",
    "            # unnormalized_marginals [numY]\n",
    "            unnormalized_marginals = [tf.reduce_prod(tf.exp(-thetas*LF_l) + 1), \\\n",
    "                                      tf.reduce_prod(tf.exp(thetas*LF_l) + 1)]\n",
    "            # same for continuous, just use the new formulation with sbins for summation, no intergration\n",
    "            Z = tf.reduce_sum(unnormalized_marginals)\n",
    "\n",
    "            # unnormalized_marginals_tiled: [numLF, numY]\n",
    "            # per-LF-marginals: [numLFs]\n",
    "            per_LF_marginals = tf.gather(unnormalized_marginals, LF_label)\n",
    "\n",
    "#             return per_LF_marginals\n",
    "            # PÎ¸(kj=y): [numLFs]  (computing Eq 14)                       \n",
    "            per_LF_agreement_prob = tf.exp(thetas)/(1 + tf.exp(thetas))*per_LF_marginals/Z\n",
    "\n",
    "#             p_thetas =  tf.expand_dims(prod__,1) * exp_terms/(1+exp_terms) / Z\n",
    "            p_thetas = per_LF_agreement_prob\n",
    "\n",
    "            ptheta_terms = a_t * n_t * tf.log(p_thetas) + (1-a_t) * n_t * tf.log(1-p_thetas)\n",
    "                                                   \n",
    "#-------------------------------\n",
    "\n",
    "            return tf.negative(tf.reduce_sum(ptheta_terms)) # add other loss component\n",
    "\n",
    "\n",
    "        loss_recall = 0\n",
    "        if(penalty > 100):\n",
    "            numYs=2\n",
    "            per_lf_prob, loss_new, marginals, per_lf_recall, pots = model_only.model(numYs, k, l, s, thetas, alphas, isdiscrete, user_a, penalty,alpha_max)\n",
    "            loss_precision = model_only.precision_loss(a_t, n_t, per_lf_prob)\n",
    "            if r_t is not None and (penalty/10) % 10 == 3:\n",
    "                loss_recall = model_only.recall_loss(r_t, n_t, per_lf_recall, isdiscrete)\n",
    "\n",
    "        prec_loss = tf.constant(0)\n",
    "    \n",
    "        if(not norm):\n",
    "            print(\"unnormlized loss\")\n",
    "            loss = tf.negative(tf.reduce_sum(lsp  ))\n",
    "        elif(penalty == 1):\n",
    "            print(\"penalty1\")\n",
    "            loss = tf.negative(tf.reduce_sum(lsp  - logz  )) \\\n",
    "                      +(g*tf.reduce_sum(tf.maximum(tf.zeros_like(thetas),-thetas)))\n",
    "        elif(penalty == 2):\n",
    "            print(\"penalty2\")\n",
    "            loss = tf.negative(tf.reduce_sum(lsp  - logz  )) \\\n",
    "                     -(g*tf.minimum( tf.reduce_min(thetas),0.0))\n",
    "            prec_loss = tf.constant(0)\n",
    "        elif(penalty == 3):\n",
    "            print(\"penalty3\")\n",
    "            loss = tf.negative(tf.reduce_sum(lsp  - logz  )) \\\n",
    "                     +(g*tf.reduce_sum(tf.log(1+tf.exp(-thetas-p3k))))\n",
    "        \n",
    "\n",
    "        elif(penalty == 4):\n",
    "            print(\"precision penalty\")\n",
    "            prec_loss = tf.reduce_sum(tf.map_fn(lambda j: softplus_p(j),np.arange(NoOfLFs),\\\n",
    "                                             dtype=tf.float64))\n",
    "            loss = tf.negative(tf.reduce_sum(lsp  - logz  )) \\\n",
    "                + (g* prec_loss)\n",
    "        \n",
    "        elif(penalty == 41):\n",
    "            print(\"precision penalty\")\n",
    "            prec_loss, xtra = sfp()\n",
    "            loss = tf.negative(tf.reduce_sum(lsp  - logz  )) \\\n",
    "                + (g * prec_loss)\n",
    "#                 + (g*tf.reduce_sum(tf.map_fn(lambda j: softplus_p(j),np.arange(NoOfLFs),\\\n",
    "#                                              dtype=tf.float64)))\n",
    "        \n",
    "        elif(penalty == 42):\n",
    "            print(\"precision penalty\")\n",
    "            temp_prloss, _ = sfpp()\n",
    "            temp_uloss = tf.negative(tf.reduce_sum(lsp  - logz  ))\n",
    "            r21 = tf.gradients(temp_uloss, thetas)\n",
    "            r22 = tf.gradients(temp_prloss, thetas)\n",
    "            xtra_tfg = tf.reduce_sum(tf.abs(r21[0][0]))/tf.reduce_sum(tf.abs(r22[0][0]))\n",
    "#             gammaR = temp_uloss/temp_prloss\n",
    "#             prec_loss, xtra = sfpp()\n",
    "            prec_loss = tf.reduce_sum(temp_prloss)\n",
    "\n",
    "            grad_ratio = 1\n",
    "\n",
    "            xtra = [xtra_tfg]\n",
    "\n",
    "            loss = temp_uloss \\\n",
    "                + (g * prec_loss)\n",
    "\n",
    "        elif(penalty == 44):\n",
    "            print(\"precision penalty\")\n",
    "            loss = tf.negative(tf.reduce_sum(lsp  - logz  )) \\\n",
    "                + (g*tf.reduce_sum(tf.map_fn(lambda j: softplus_pp(j),np.arange(NoOfLFs),\\\n",
    "                                             dtype=tf.float64)))\n",
    "                \n",
    "                \n",
    "        elif(penalty == 5):\n",
    "            print(\"precision log(softplus) penalty\")\n",
    "            loss = tf.negative(tf.reduce_sum(lsp  - logz  )) \\\n",
    "                + (g*tf.reduce_sum(tf.map_fn(lambda j: tf.log(softplus_p(j)),np.arange(NoOfLFs),\\\n",
    "                                             dtype=tf.float64)))\n",
    "        elif(penalty == 6):\n",
    "            print(\"recall penalty\")\n",
    "            loss = tf.negative(tf.reduce_sum(lsp  - logz  )) \\\n",
    "                + (g*tf.reduce_sum(tf.map_fn(lambda j: softplus_r(j),np.arange(NoOfLFs),\\\n",
    "                                             dtype=tf.float64)))\n",
    "        elif(penalty == 7):\n",
    "            print(\"precision and recall penalty\")\n",
    "            loss = tf.negative(tf.reduce_sum(lsp  - logz  )) \\\n",
    "                + (g*tf.reduce_sum(tf.map_fn(lambda j: softplus_p(j),np.arange(NoOfLFs),\\\n",
    "                                             dtype=tf.float64))) \\\n",
    "                + (g*tf.reduce_sum(tf.map_fn(lambda j: softplus_r(j),np.arange(NoOfLFs),\\\n",
    "                                             dtype=tf.float64)))\n",
    "        elif(penalty == 8):\n",
    "            print(\"precision and sign 1 penalty\")\n",
    "            loss = tf.negative(tf.reduce_sum(lsp  - logz  )) \\\n",
    "                + (g*tf.reduce_sum(tf.map_fn(lambda j: softplus_p(j),np.arange(NoOfLFs),\\\n",
    "                                             dtype=tf.float64))) \\\n",
    "                + (g*tf.reduce_sum(tf.maximum(tf.zeros_like(thetas),-thetas)))\n",
    "        elif(penalty == 9):\n",
    "            print(\"precision and sign 2 penalty\")\n",
    "            loss = tf.negative(tf.reduce_sum(lsp  - logz  )) \\\n",
    "                + (g*tf.reduce_sum(tf.map_fn(lambda j: softplus_p(j),np.arange(NoOfLFs),\\\n",
    "                                             dtype=tf.float64))) \\\n",
    "                - (g*tf.minimum( tf.reduce_min(thetas),0.0))\n",
    "        elif(penalty == 10):\n",
    "            print(\"precision and sign 3 penalty\")\n",
    "            loss = tf.negative(tf.reduce_sum(lsp  - logz  )) \\\n",
    "                + (g*tf.reduce_sum(tf.map_fn(lambda j: softplus_p(j),np.arange(NoOfLFs),\\\n",
    "                                             dtype=tf.float64))) \\\n",
    "                + (g*tf.reduce_sum(tf.log(1+tf.exp(-thetas-p3k))))\n",
    "        elif(penalty == 11):\n",
    "            print(\"precision and recall and sign 2 penalty\")\n",
    "            loss = tf.negative(tf.reduce_sum(lsp  - logz  )) \\\n",
    "                + (g*tf.reduce_sum(tf.map_fn(lambda j: softplus_p(j),np.arange(NoOfLFs),\\\n",
    "                                             dtype=tf.float64))) \\\n",
    "                + (g*tf.reduce_sum(tf.map_fn(lambda j: softplus_r(j),np.arange(NoOfLFs),\\\n",
    "                                             dtype=tf.float64))) \\\n",
    "                  - (g*tf.minimum( tf.reduce_min(thetas),0.0))\n",
    "        elif(penalty == 12):\n",
    "            print(\"recall and sign 2 penalty\")\n",
    "            loss = tf.negative(tf.reduce_sum(lsp  - logz  )) \\\n",
    "                + (g*tf.reduce_sum(tf.map_fn(lambda j: softplus_r(j),np.arange(NoOfLFs),\\\n",
    "                                             dtype=tf.float64))) \\\n",
    "                  - (g*tf.minimum( tf.reduce_min(thetas),0.0))\n",
    "        elif(penalty == 15):\n",
    "            print(\"equation 15 and sign 2 penalty\")\n",
    "            loss = tf.negative(tf.reduce_sum(lsp  - logz  )) \\\n",
    "                + (g*tf.reduce_sum(tf.map_fn(lambda j: softplus_r15(j),np.arange(NoOfLFs),\\\n",
    "                                             dtype=tf.float64))) \\\n",
    "                  - (g*tf.minimum( tf.reduce_min(thetas),0.0))\n",
    "\n",
    "\n",
    "\n",
    "                    \n",
    "        elif(penalty == 20):\n",
    "            print(\"new model\")\n",
    "            prec_loss = get_loss_20()\n",
    "            loss =  tf.negative(tf.reduce_sum(lsp  - logz  )) + prec_loss\n",
    "                  \n",
    "        elif(penalty >100 and penalty<111):\n",
    "            sys.stdout = stdout_saved\n",
    "            print(\"new model smooth \", penalty)\n",
    "            sys.stdout = open(\"trash\", \"w\")            \n",
    "            print(\"new model smooth \", penalty)\n",
    "            loss = loss_new\n",
    "#             loss, _ = smooth_new_prec()\n",
    "            \n",
    "                    \n",
    "        elif(penalty >110):\n",
    "            sys.stdout = stdout_saved\n",
    "            print(\"new model smooth with constraints \", penalty)\n",
    "            sys.stdout = open(\"trash\", \"w\")            \n",
    "            print(\"new model smooth with constraints \", penalty)\n",
    "            loss = loss_new + loss_precision + loss_recall\n",
    "#             loss_un, loss_pr = smooth_new_prec()\n",
    "#             loss = loss_un + loss_pr\n",
    "\n",
    "        else:\n",
    "            print(\"normalized loss\")\n",
    "            loss = tf.negative(tf.reduce_sum(lsp  - logz  ))\n",
    "            prec_loss = tf.constant(0)\n",
    "       \n",
    "        if(debug):\n",
    "            print(\"loss\",loss)\n",
    "\n",
    "#         marginals = tf.nn.softmax(t_pout,axis=0)\n",
    "\n",
    "        if(debug):\n",
    "            print(\"marginals\",marginals)\n",
    "        predict = tf.argmax(marginals,axis=0)\n",
    "\n",
    "\n",
    "        train_step = tf.train.AdamOptimizer(lr).minimize(loss)#, var_list=[thetas, alphas]) \n",
    "\n",
    "\n",
    "        init_g = tf.global_variables_initializer()\n",
    "        init_l = tf.local_variables_initializer()\n",
    "        \n",
    "        dev_high = np.zeros(3)\n",
    "        test_high = np.zeros(3)\n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init_g)\n",
    "            sess.run(init_l)\n",
    "\n",
    "            flg = False\n",
    "            # Initialize an iterator over the training dataset.\n",
    "            for en in range(ep):\n",
    "                sess.run(train_init_op)\n",
    "#                 temprun = sess.run(next_element2)\n",
    "#                 print('len next ',len(temprun), len(temprun[0]))\n",
    "#                 print(temprun[0])\n",
    "                tl = 0\n",
    "                pll = 0\n",
    "                unl = 0\n",
    "                try:\n",
    "                    it = 0\n",
    "                    \n",
    "                    grad_sum = 0.0\n",
    "                    while True:\n",
    "#                         print(en, it)\n",
    "\n",
    "                        _,ls,ploss,xpp,t = sess.run([train_step,loss,prec_loss,xtra,thetas])\n",
    "                        if(penalty == 42):\n",
    "                            un_loss = sess.run(temp_uloss)\n",
    "\n",
    "                        tl = tl + ls\n",
    "                        pll = pll + ploss\n",
    "                        if(penalty == 42):\n",
    "                            unl = unl + un_loss\n",
    "                        it = it + 1\n",
    "#                         newptheta = sess.run(sbins)\n",
    "#                         ztemp = sess.run(xyz)\n",
    "                \n",
    "#                         print(it*BATCH_SIZE)\n",
    "                \n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    pass\n",
    "                opstring = \"\"\n",
    "                print(en,\"loss\",tl)\n",
    "                opstring = opstring + str(en) + \",\" + str(tl) + \",\"\n",
    "#                 print('temp__', temp__)\n",
    "                print(\"un loss\", unl)\n",
    "                print(\"ploss\", pll)\n",
    "                print(\"ratio\", unl/pll)\n",
    "                print(\"extra\", xpp)\n",
    "#                 dloss = sess.run(dev_loss)\n",
    "#                 print(\"dloss\", dloss)\n",
    "#                 print(\"expected ratio\", xpp/(xpp+1))\n",
    "                print(\"dev set\")\n",
    "                sess.run(dev_init_op)\n",
    "##                 sm,a,t,m,pl = sess.run([summary_merged,alphas,thetas,marginals,predict])\n",
    "#                 a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "                a,t,m,pl,ddloss = sess.run([alphas,thetas,marginals,predict,loss])\n",
    "##                 test_writer.add_summary(sm, en)\n",
    "                print('init dev loss', ddloss)\n",
    "                opstring = opstring + str(ddloss) + \",\"\n",
    "                print(a)\n",
    "                print(t)\n",
    "                unique, counts = np.unique(pl, return_counts=True)\n",
    "                print(dict(zip(unique, counts)))\n",
    "                print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "                dev_results = precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\")\n",
    "                print('devresults ', dev_results)\n",
    "                roc_dev = roc_auc_score(np.array(gold_labels_dev), np.array(m[1,:,0]))\n",
    "                print('roc score', roc_dev)\n",
    "                opstring = str(en) + ',' #do not print losses\n",
    "                opstring = opstring + str(dev_results[2]) + \",\" + str(roc_dev) + \",\"\n",
    "#                 if(dev_results[2]>dev_high[2]):\n",
    "                dev_high = np.array(dev_results)\n",
    "#                 dev_high = max(dev_high, dev_results[2])\n",
    "                print()\n",
    "                print(\"test set\")\n",
    "                sess.run(test_init_op)\n",
    "                a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "                unique, counts = np.unique(pl, return_counts=True)\n",
    "                print(dict(zip(unique, counts)))\n",
    "                print(\"acc\",accuracy_score(gold_labels_test,pl))\n",
    "                test_results = precision_recall_fscore_support(np.array(gold_labels_test),np.array(pl),average=\"binary\")\n",
    "                print('testresults ', test_results)\n",
    "                roc_test = roc_auc_score(np.array(gold_labels_test), np.array(m[1,:,0]))\n",
    "                print('roc score', roc_test)\n",
    "                opstring = opstring + str(test_results[2]) + \",\" + str(roc_test)\n",
    "#                 if(test_results[2]>test_high[2]):\n",
    "                test_high = np.array(test_results)\n",
    "                print()\n",
    "        \n",
    "                sys.stdout = stdout_saved\n",
    "                print(opstring)\n",
    "# #                 newptheta = sess.run(marginals)\n",
    "#                 print(\"new z \", ztemp)\n",
    "#                 print(\"new z shape \", np.array(ztemp).shape)\n",
    "#                 print('shape ',np.array(newptheta).shape)\n",
    "                sys.stdout = open(\"trash\", \"w\")\n",
    "                \n",
    "\n",
    "    return pl, m, dev_high, opstring#test_high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ 116\n",
      "new model smooth with constraints  116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sunita/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:379: RuntimeWarning: invalid value encountered in long_scalars\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [200, 1]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-0112902237c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m#        sys.stdout = stdout_saved\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         _, _, _, ops = train2(1/len(train_L_S),200,batch_size = b, th = tf.truncated_normal_initializer(0,0.1,seed),                                    af = tf.truncated_normal_initializer(0,0.1,seed),                                    user_a = ua,                                    LF_acc = np.repeat(0.4, len(LF_l)) ,pcl=np.array([-1,1],dtype=np.float64),                                    norm=True,smooth=True,penalty=pen_i, Gamma=gg, debug=False,                                    a_pr = np.repeat(0.65, len(LF_l)),                                        n_pr = np.repeat(100, len(LF_l)),                                     isdiscrete = dscr, alpha_max=np.repeat(0.9, len(LF_l)),\n\u001b[0;32m---> 26\u001b[0;31m                              r_pr = np.repeat(0.750, len(LF_l)))\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0morig_stdout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# get_LF_acc(dev_L_S,gold_labels_dev)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-8dd0ac636fcf>\u001b[0m in \u001b[0;36mtrain2\u001b[0;34m(lr, ep, th, af, batch_size, LF_acc, LF_rec, pcl, norm, smooth, penalty, p3k, alp, Gamma, debug, r1, r2, a_pr, n_pr, user_a, isdiscrete, alpha_max, r_pr)\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0munique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_counts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"acc\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgold_labels_dev\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m                 \u001b[0mdev_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprecision_recall_fscore_support\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgold_labels_dev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'devresults '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sunita/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'multilabel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sunita/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \"\"\"\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sunita/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 235\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [200, 1]"
     ]
    }
   ],
   "source": [
    "#added 2\n",
    "# import sys\n",
    "# sys.path.append(\"../../../models\")\n",
    "# from new_01 import train2\n",
    "\n",
    "a = 0.60\n",
    "ua = np.repeat(a, len(LF_l))\n",
    "dscr = np.repeat(data_discrete, len(LF_l))\n",
    "\n",
    "# import sys\n",
    "# stdout_saved = sys.stdout\n",
    "stdout_saved = orig_stdout\n",
    "# for rx in [1, len(LF_l), len(train_L_S)]:\n",
    "# for i in np.linspace(1,1,1):\n",
    "b = 32\n",
    "for pen_i in [116]:#[0, 4, 42]:\n",
    "# for b in [32]:#[32,64,128,512,1024,2048,4096]:\n",
    "    devmax = np.zeros(3)\n",
    "    testmax = np.zeros(3)\n",
    "    for gg in range(1):#[1]:#np.linspace(0,100,101):#, 10, 30, 50, 100]:\n",
    "#         print(\"batch-size:\",b, \"gamma:\",gg)\n",
    "        print('============', pen_i)\n",
    "        sys.stdout = open('trash', 'w')\n",
    "#        sys.stdout = stdout_saved\n",
    "        _, _, _, ops = train2(1/len(train_L_S),200,batch_size = b, th = tf.truncated_normal_initializer(0,0.1,seed),\\\n",
    "                                    af = tf.truncated_normal_initializer(0,0.1,seed),\\\n",
    "                                    user_a = ua,\\\n",
    "                                    LF_acc = np.repeat(0.4, len(LF_l)) ,pcl=np.array([-1,1],dtype=np.float64),\\\n",
    "                                    norm=True,smooth=True,penalty=pen_i, Gamma=gg, debug=False,\\\n",
    "                                    a_pr = np.repeat(0.65, len(LF_l)), \\\n",
    "                                       n_pr = np.repeat(100, len(LF_l)),\\\n",
    "                                     isdiscrete = dscr, alpha_max=np.repeat(0.9, len(LF_l)),\n",
    "                             r_pr = np.repeat(0.750, len(LF_l)))\n",
    "        sys.stdout = orig_stdout\n",
    "# get_LF_acc(dev_L_S,gold_labels_dev)\n",
    "#                 if(devmax_t[2]>devmax[2]):\n",
    "#         devmax = np.array(devmax_t)\n",
    "#                 if(testmax_t[2]>testmax[2]):\n",
    "#         testmax = np.array(testmax_t)\n",
    "#         r42d.append(devmax)\n",
    "#         r42t.append(testmax)\n",
    "#         print(\"\\n\\nfinal_result_line: \",\"thetas:\",0,0.1,\"batch size\",b,\"penalty\",pen_i,)\n",
    "##        sys.stdout = stdout_saved\n",
    "#         print(\"\\n\\nfinal_result_line: \",\"thetas:\",0,0.1,\"batch size\",b,\"penalty\",pen_i,)\n",
    "# sys.stdout = open('opfile2', 'a')\n",
    "# print('rate = 1/train_len')      \n",
    "# sys.stdout = stdout_saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
