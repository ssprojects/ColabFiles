{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#call this only once for a kernel startup\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "# BATCH_SIZE = 32\n",
    "seed = 12\n",
    "import sys\n",
    "orig_stdout = sys.stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 2625, 1: 189}\n",
      "{0: 2484, 1: 218}\n",
      "2814 2702\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "sys.path.append(\"../\")\n",
    "import model_only\n",
    "\n",
    "gold_labels_dev = np.load(\"data/true_labels_dev.npy\")\n",
    "gold_labels_test = np.load(\"data/true_labels_test.npy\")\n",
    "\n",
    "unique, counts = np.unique(gold_labels_dev, return_counts=True)\n",
    "print(dict(zip(unique, counts)))\n",
    "\n",
    "unique, counts = np.unique(gold_labels_test, return_counts=True)\n",
    "print(dict(zip(unique, counts)))\n",
    "\n",
    "print(len(gold_labels_dev),len(gold_labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def draw2DArray(a):\n",
    "    fig = plt.figure(figsize=(6, 3.2))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_title('colorMap')\n",
    "    plt.imshow(np.array(a))\n",
    "    ax.set_aspect('equal')\n",
    "    cax = fig.add_axes([0.12, 0.1, 0.78, 0.8])\n",
    "    cax.get_xaxis().set_visible(False)\n",
    "    cax.get_yaxis().set_visible(False)\n",
    "    cax.patch.set_alpha(0)\n",
    "    cax.set_frame_on(False)\n",
    "    plt.colorbar(orientation='vertical')\n",
    "    plt.show()\n",
    "    \n",
    "      \n",
    "def report2dict(cr):\n",
    "    # Parse rows\n",
    "    tmp = list()\n",
    "    for row in cr.split(\"\\n\"):\n",
    "        parsed_row = [x for x in row.split(\"  \") if len(x) > 0]\n",
    "        if len(parsed_row) > 0:\n",
    "            tmp.append(parsed_row)\n",
    "    \n",
    "    # Store in dictionary\n",
    "    measures = tmp[0]\n",
    "\n",
    "    D_class_data = defaultdict(dict)\n",
    "    for row in tmp[1:]:\n",
    "        class_label = row[0]\n",
    "        for j, m in enumerate(measures):\n",
    "            D_class_data[class_label][m.strip()] = float(row[j + 1].strip())\n",
    "    return pd.DataFrame(D_class_data).T\n",
    "\n",
    "def predictAndPrint(pl):\n",
    "    print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "    print(precision_recall_fscore_support(gold_labels_test,pl,average='binary'))\n",
    "    print(confusion_matrix(gold_labels_dev,pl))\n",
    "#     draw2DArray(confusion_matrix(gold_labels_dev,pl))\n",
    "#     return report2dict(classification_report(gold_labels_dev, pl))# target_names=class_names))\n",
    "    \n",
    "\n",
    "\n",
    "def drawPRcurve(y_test,y_score,it_no):\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    splt = fig.add_subplot(111)\n",
    "    \n",
    "\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_score,pos_label=1)\n",
    "\n",
    "    splt.step(recall, precision, color='b', alpha=0.2,\n",
    "             where='post')\n",
    "    splt.fill_between(recall, precision, step='post', alpha=0.2,\n",
    "                     color='b')\n",
    "    average_precision = average_precision_score(y_test, y_score)\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlim([0.0, 1.05])\n",
    "    plt.title('{0:d} Precision-Recall curve: AP={1:0.2f}'.format(it_no,\n",
    "              average_precision))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LF_l = [\n",
    "    1,1,1,1,1,-1,1,-1,-1,-1\n",
    "]\n",
    "\n",
    "def merge(a,b):\n",
    "    c = []\n",
    "    for i in range(len(a)):\n",
    "        ci = []\n",
    "        ci_l = a[i,0,:].tolist()+b[i,0,:].tolist()\n",
    "        ci_s = a[i,1,:].tolist()+b[i,1,:].tolist()\n",
    "        ci.append(ci_l)\n",
    "        ci.append(ci_s)\n",
    "        c.append(ci)\n",
    "    return c\n",
    "import numpy as np\n",
    "dev_L_S_s = np.load(\"data/dev_L_S_smooth.npy\")\n",
    "test_L_S_s = np.load(\"data/test_L_S_smooth.npy\")\n",
    "train_L_S_s = np.load(\"data/train_L_S_smooth.npy\")\n",
    "\n",
    "dev_L_S_d = np.load(\"data/dev_L_S_discrete.npy\")\n",
    "test_L_S_d = np.load(\"data/test_L_S_discrete.npy\")\n",
    "train_L_S_d = np.load(\"data/train_L_S_discrete.npy\")\n",
    "\n",
    "dev_L_S = np.array(merge(dev_L_S_d,dev_L_S_s))\n",
    "train_L_S = np.array(merge(train_L_S_d,train_L_S_s))\n",
    "test_L_S = np.array(merge(test_L_S_d,test_L_S_s))\n",
    "\n",
    "LF_l = LF_l + LF_l\n",
    "NoOfLFs= len(LF_l)\n",
    "\n",
    "print(len(LF_l))\n",
    "\n",
    "print(dev_L_S.shape, test_L_S.shape, train_L_S.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2814 2702\n",
      "(2702, 2, 10) (2814, 2, 10) (22276, 2, 10)\n"
     ]
    }
   ],
   "source": [
    "LF_l = [\n",
    "    1,1,1,1,1,-1,1,-1,-1,-1\n",
    "]\n",
    "NoOfLFs= len(LF_l)\n",
    "\n",
    "dev_L_S = np.load(\"data/dev_L_S_smooth.npy\")\n",
    "test_L_S = np.load(\"data/test_L_S_smooth.npy\")\n",
    "train_L_S = np.load(\"data/train_L_S_smooth.npy\")\n",
    "\n",
    "print(len(gold_labels_dev),len(gold_labels_test))\n",
    "print(test_L_S.shape,dev_L_S.shape,train_L_S.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LF_l = [\n",
    "    1,1,1,1,1,-1,1,-1,-1,-1\n",
    "]\n",
    "NoOfLFs= len(LF_l)\n",
    "\n",
    "dev_L_S = np.load(\"data/dev_L_S_discrete.npy\")\n",
    "test_L_S = np.load(\"data/test_L_S_discrete.npy\")\n",
    "train_L_S = np.load(\"data/train_L_S_discrete.npy\")\n",
    "\n",
    "print(len(gold_labels_dev),len(gold_labels_test))\n",
    "print(test_L_S.shape,dev_L_S.shape,train_L_S.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          1.          1.          0.         -1.\n",
      "   0.         -1.         -1.         -1.        ]\n",
      " [ 0.5         1.          0.83758436  0.83758436  0.5         0.875\n",
      "   0.5         1.          1.          0.72379396]]\n"
     ]
    }
   ],
   "source": [
    "print(train_L_S[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \t 2 {0.0: 22243, 1.0: 33}\n",
      "1 \t 2 {0.0: 22096, 1.0: 180}\n",
      "2 \t 2473 \n",
      "3 \t 3582 \n",
      "4 \t 2 {0.0: 21904, 1.0: 372}\n",
      "5 \t 2 {-0.875: 13427, 0.0: 8849}\n",
      "6 \t 1751 \n",
      "7 \t 2477 \n",
      "8 \t 3581 \n",
      "9 \t 2689 \n",
      "[1, 1, 0, 0, 1, 1, 0, 0, 0, 0]"
     ]
    }
   ],
   "source": [
    "discr = []\n",
    "for j in range(len(LF_l)):\n",
    "    temp = np.array(train_L_S[:,0,j]*np.array(train_L_S[:,1,j]))\n",
    "    unique, counts = np.unique(temp, return_counts=True)\n",
    "    temp_d = dict(zip(unique, counts))\n",
    "    print(j, '\\t', len(temp_d), temp_d if len(temp_d)==2 else '')\n",
    "    discr.append(int(not(len(temp_d)-2)))\n",
    "sys.stdout.write('[')\n",
    "for x in range(len(discr)-1):\n",
    "    sys.stdout.write(str(discr[x]) + ', ')\n",
    "sys.stdout.write(str(discr[-1])+']')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "test\n"
     ]
    }
   ],
   "source": [
    "NoOfClasses = 2\n",
    "print(len(LF_l))\n",
    "\n",
    "sys.stdout = orig_stdout\n",
    "print('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ 101\n",
      "new model smooth  101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinay/snorkelEnv/lib/python3.5/site-packages/ipykernel_launcher.py:456: RuntimeWarning: invalid value encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,0.3966244725738397,0.7624902998236333,0.46232876712328763,0.8036359305056953\n",
      "1,0.405727923627685,0.7412980599647268,0.4883720930232558,0.7880314748334294\n",
      "2,0.27027027027027023,0.7081894683799446,0.3459715639810427,0.7541504158725938\n",
      "3,0.26666666666666666,0.7027936507936507,0.3501199040767386,0.7501892848173262\n",
      "4,0.25882352941176473,0.7010138573948097,0.34192037470725994,0.7492936444621725\n",
      "5,0.25139664804469275,0.6996130007558579,0.3251670378619154,0.7496685207345359\n",
      "6,0.2513661202185793,0.699092970521542,0.3191489361702127,0.7527136979420584\n",
      "7,0.24731182795698925,0.6991917359536406,0.3125,0.7527155446232032\n",
      "8,0.37053571428571436,0.7241713277903754,0.4382608695652174,0.7768553605460267\n",
      "9,0.3644251626898048,0.7229155958679767,0.4300341296928328,0.7774149049328547\n",
      "10,0.35294117647058826,0.7250138573948097,0.4110929853181076,0.7774185982951439\n",
      "11,0.3533697632058288,0.7361421012849585,0.4063860667634252,0.789826448906026\n",
      "12,0.35862068965517246,0.7481914840010078,0.4175824175824176,0.8013977529583832\n",
      "13,0.3486842105263157,0.7510657596371881,0.4084880636604774,0.8039480196191404\n",
      "14,0.3428571428571428,0.7537666918619299,0.4015345268542199,0.8074585604751141\n",
      "15,0.33485540334855407,0.7584368858654573,0.390547263681592,0.8097576785001994\n",
      "16,0.33137829912023464,0.7677853363567649,0.37831325301204816,0.8082821802656266\n",
      "17,0.31923601637107774,0.7703562610229276,0.3669724770642202,0.8102119620617826\n",
      "18,0.30808729139922975,0.7712290249433107,0.35698447893569846,0.8137557431783599\n",
      "19,0.29666254635352285,0.7710919627110103,0.34773218142548595,0.8150650401099145\n",
      "20,0.2823803967327888,0.7729533887629125,0.32525252525252524,0.8146329167220671\n",
      "21,0.2722159730033746,0.7712733686067019,0.31702544031311153,0.8168803276751024\n",
      "22,0.2662266226622662,0.7714396573444193,0.31094049904030713,0.8177888947982686\n",
      "23,0.262757871878393,0.7701345427059714,0.3094555873925501,0.818052970201953\n",
      "24,0.2601279317697228,0.7686832955404385,0.3071090047393365,0.8174509521487983\n",
      "25,0.26004228329809725,0.7687649281934996,0.30537229029217716,0.8168286206030522\n",
      "26,0.26105263157894737,0.767949609473419,0.30280373831775703,0.81765039371242\n",
      "27,0.2646153846153846,0.7710949861426053,0.3076923076923077,0.8203336214155919\n",
      "28,0.2595870206489675,0.7718951877047115,0.3023872679045093,0.8209559529613378\n",
      "29,0.2520852641334569,0.7728364827412446,0.2951653944020356,0.8215431975653356\n",
      "30,0.24438454627133874,0.7716573444192492,0.2857142857142857,0.8203262346910134\n",
      "31,0.2318840579710145,0.7704439405391786,0.2720875684128225,0.819377040582665\n",
      "32,0.23154362416107385,0.7712683295540438,0.27027027027027023,0.819554321972551\n",
      "33,0.23267326732673266,0.7733565129755606,0.2677760968229954,0.820756511397716\n",
      "34,0.22889610389610385,0.7731368102796674,0.26646706586826346,0.821190481466708\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-629ca34a67e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'trash'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m#        sys.stdout = stdout_saved\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_L_S\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtruncated_normal_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m                                    \u001b[0maf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtruncated_normal_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m                                    \u001b[0muser_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mua\u001b[0m\u001b[0;34m,\u001b[0m                                    \u001b[0mLF_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLF_l\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mpcl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m                                    \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msmooth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpen_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m                                    \u001b[0ma_pr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.650\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLF_l\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m                                            \u001b[0mn_pr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLF_l\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m                                     \u001b[0misdiscrete\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdscr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0morig_stdout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# get_LF_acc(dev_L_S,gold_labels_dev)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-2b4338f3dd37>\u001b[0m in \u001b[0;36mtrain2\u001b[0;34m(lr, ep, th, af, batch_size, LF_acc, LF_rec, pcl, norm, smooth, penalty, p3k, alp, Gamma, debug, r1, r2, a_pr, n_pr, user_a, isdiscrete)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[0;31m#                         print(en, it)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m                         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mls\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mploss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxpp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprec_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxtra\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mthetas\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m                         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpenalty\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m                             \u001b[0mun_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_uloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/snorkelEnv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/snorkelEnv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1140\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/snorkelEnv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1321\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/snorkelEnv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/snorkelEnv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1312\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/snorkelEnv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[1;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m             status, run_metadata)\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    }
   ],
   "source": [
    "#added 2\n",
    "r42d = []\n",
    "r42t = []\n",
    "\n",
    "a = 0.70\n",
    "\n",
    "ua = np.repeat(a, len(LF_l))\n",
    "dscr = np.array([1,1,0,0,1,1,0,0,0,0])\n",
    "# dscr = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0]\n",
    "# dscr = np.ones(10)\n",
    "\n",
    "stdout_saved = orig_stdout\n",
    "# for rx in [1, len(LF_l), len(train_L_S)]:\n",
    "# for i in np.linspace(1,1,1):\n",
    "b = 32\n",
    "for pen_i in [101,102,103,104,105,106,111,112,113,114,115,116]:#[0, 4, 42]:\n",
    "# for b in [32]:#[32,64,128,512,1024,2048,4096]:\n",
    "    devmax = np.zeros(3)\n",
    "    testmax = np.zeros(3)\n",
    "    for gg in range(1):#[1]:#np.linspace(0,100,101):#, 10, 30, 50, 100]:\n",
    "#         print(\"batch-size:\",b, \"gamma:\",gg)\n",
    "        print('============', pen_i)\n",
    "        sys.stdout = open('trash', 'w')\n",
    "#        sys.stdout = stdout_saved\n",
    "        _, _, _, ops = train2(1/len(train_L_S),200,batch_size = b, th = tf.truncated_normal_initializer(1,0,seed),\\\n",
    "                                    af = tf.truncated_normal_initializer(0,0.1,seed),\\\n",
    "                                    user_a = ua,\\\n",
    "                                    LF_acc = np.repeat(0.4, len(LF_l)) ,pcl=np.array([-1,1],dtype=np.float64),\\\n",
    "                                    norm=True,smooth=True,penalty=pen_i, Gamma=gg, debug=False,\\\n",
    "                                    a_pr = np.repeat(0.650, len(LF_l)), \\\n",
    "                                           n_pr = np.repeat(4000, len(LF_l)),\\\n",
    "                                     isdiscrete = dscr)\n",
    "        sys.stdout = orig_stdout\n",
    "# get_LF_acc(dev_L_S,gold_labels_dev)\n",
    "#                 if(devmax_t[2]>devmax[2]):\n",
    "#         devmax = np.array(devmax_t)\n",
    "#                 if(testmax_t[2]>testmax[2]):\n",
    "#         testmax = np.array(testmax_t)\n",
    "#         r42d.append(devmax)\n",
    "#         r42t.append(testmax)\n",
    "#         print(\"\\n\\nfinal_result_line: \",\"thetas:\",0,0.1,\"batch size\",b,\"penalty\",pen_i,)\n",
    "##        sys.stdout = stdout_saved\n",
    "#         print(\"\\n\\nfinal_result_line: \",\"thetas:\",0,0.1,\"batch size\",b,\"penalty\",pen_i,)\n",
    "# sys.stdout = open('opfile2', 'a')\n",
    "# print('rate = 1/train_len')      \n",
    "# sys.stdout = stdout_saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#added new model\n",
    "def train2(lr,ep,th,af,batch_size=32,LF_acc=None,LF_rec=None,pcl=np.array([-1,1],dtype=np.float64),norm=True,\\\n",
    "          smooth=True,penalty=0,p3k=3,alp=1,Gamma=1.0,debug=True, \\\n",
    "           r1=1.0, r2=1.0, a_pr=None, n_pr=None, user_a = None, isdiscrete = None):\n",
    "    \n",
    "    ## lr : learning rate\n",
    "    ## ep : no of epochs\n",
    "    ## th : thetas initializer\n",
    "    ## af : alphas initializer\n",
    "    ## penalty : {1,2,3} use one of the three penalties, 0: no-penalty\n",
    "    ## p3k : parameter for penalty-3 \n",
    "    ## smooth : flag if smooth lfs are used \n",
    "    ## make sure smooth/discrete LF data is loaded into train_L_S and test_L_S\n",
    "    ## pcl : all possible class labels  = [-1,1] for binary, \n",
    "    ##       np.arange(0,NoOfClasses) for multiclass\n",
    "    ## alp : alpha parameter (to set a max value for alpha)\n",
    "    ## norm : use normalization or not\n",
    "    ## Gamma : penalty tuning parameter\n",
    "    \n",
    "    BATCH_SIZE = batch_size\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    seed = 12\n",
    "    \n",
    "    LF_k = []\n",
    "    if(len(pcl)==2):\n",
    "        for i in range(len(LF_l)):\n",
    "            LF_k.append(int((LF_l[i]+1)/2))\n",
    "    with tf.Graph().as_default():\n",
    "        xtra = tf.Variable(0, trainable=False)\n",
    "        gammaR = tf.constant(0, dtype=tf.float64)\n",
    "\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices(train_L_S).batch(BATCH_SIZE)\n",
    "        dev_dataset = tf.data.Dataset.from_tensor_slices(dev_L_S).batch(len(dev_L_S))\n",
    "        test_dataset = tf.data.Dataset.from_tensor_slices(test_L_S).batch(len(test_L_S))\n",
    "\n",
    "        iterator = tf.data.Iterator.from_structure(train_dataset.output_types, train_dataset.output_shapes)\n",
    "        next_element = iterator.get_next()\n",
    "\n",
    "        train_init_op = iterator.make_initializer(train_dataset)\n",
    "        dev_init_op = iterator.make_initializer(dev_dataset)\n",
    "        test_init_op = iterator.make_initializer(test_dataset)\n",
    "\n",
    "        next_element = iterator.get_next()\n",
    "\n",
    "        alphas = tf.get_variable('alphas', [NoOfLFs], initializer=af, dtype=tf.float64)\n",
    "        if(penalty in [103, 113]):\n",
    "            alphas = tf.stop_gradient(alphas*0)\n",
    "            \n",
    "        thetas = tf.get_variable('thetas', [1,NoOfLFs], initializer=th, dtype=tf.float64)\n",
    "        \n",
    "        LF_label = tf.convert_to_tensor(LF_k, dtype=tf.int32)\n",
    "        k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
    "        g = tf.convert_to_tensor(Gamma, dtype=tf.float64)\n",
    "        \n",
    "        if(penalty in [4,5,7,8,9,10,11, 44, 42, 41]):\n",
    "            LF_a = tf.convert_to_tensor(LF_acc, dtype=tf.float64)\n",
    "        \n",
    "        if(penalty in [6,7,11,12,15]):\n",
    "            LF_r = tf.convert_to_tensor(LF_rec, dtype=tf.float64)\n",
    "        \n",
    "        l,s =  tf.unstack(next_element,axis=1)\n",
    "\n",
    "        if(smooth and penalty < 100):\n",
    "            s_ = tf.maximum(tf.subtract(s,tf.minimum(alphas,alp)), 0)\n",
    "            if(debug):\n",
    "                print(\"s_\",s_)\n",
    "\n",
    "        def iskequalsy(v,s):\n",
    "            out = tf.where(tf.equal(v,s),tf.ones_like(v),-tf.ones_like(v))\n",
    "            if(debug):\n",
    "                print(\"out\",out)\n",
    "            return out\n",
    "#MAP\n",
    "        if(penalty < 100):\n",
    "            if(smooth):\n",
    "                pout = tf.map_fn(lambda c: l*c*s_ ,pcl,name=\"pout\")\n",
    "            else:\n",
    "                pout = tf.map_fn(lambda c: l*c ,pcl,name=\"pout\")\n",
    "\n",
    "            t =  tf.squeeze(thetas)                \n",
    "                \n",
    "            def ints(y): # called for y=1 and y=-1\n",
    "                ky = iskequalsy(k,y)  # ky = 1 if k==y else -1\n",
    "                if(debug):\n",
    "                    print(\"ky\",ky)\n",
    "                out1 = alphas+((tf.exp((t*ky*(1-alphas)))-1)/(t*ky))\n",
    "                if(debug):\n",
    "                    print(\"intsy\",out1)\n",
    "                return out1                \n",
    "                \n",
    "            if(smooth):\n",
    "                #smooth normalizer\n",
    "                zy = tf.map_fn(lambda y: tf.reduce_prod(1+ints(y),axis=0),\\\n",
    "                               pcl,name=\"zy\")\n",
    "            else:\n",
    "                #discrete normalizer\n",
    "                zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t*iskequalsy(k,y)),axis=0),\\\n",
    "                               pcl,name=\"zy\")            \n",
    "#new model\n",
    "\n",
    "        if(penalty > 200):\n",
    "\n",
    "#             z_y_new = [tf.reduce_prod(1 + 1/(-1*k*thetas) * (tf.exp(-1*k*thetas) - tf.exp(-1*k*thetas*user_alphas)) ), \\\n",
    "#                    tf.reduce_prod(1 + 1/(1*k*thetas) * (tf.exp(1*k*thetas) - tf.exp(1*k*thetas*user_alphas)) ) ]\n",
    "            numbins = tf.constant(10, dtype=tf.float64)\n",
    "            #TODO: 2 binned integration over the s values to allow stable gradients.\n",
    "            sbin_widths = (1-user_alphas)/numbins\n",
    "            sbins = tf.einsum('i,j->ij', sbin_widths, tf.cast(tf.range(0,numbins+1), dtype=tf.float64))+tf.expand_dims(user_alphas,1)\n",
    "            sbins = tf.transpose(sbins)\n",
    "            \n",
    "            def pot(s):\n",
    "                if(penalty%10 == 1):\n",
    "                    return thetas*s-alphas\n",
    "                elif(penalty%10 == 2):\n",
    "                    return thetas*tf.clip_by_value(s-alphas,0,1)\n",
    "                elif(penalty%10 == 3):\n",
    "                    return thetas*s\n",
    "                return 0\n",
    "                                       \n",
    "            # over the y-s it is okay to use a map function, but not over the LFs.\n",
    "            z_y_new = tf.map_fn(lambda y: tf.reduce_prod(1 + tf.reduce_sum(tf.exp(-y*k*pot(sbins)), axis=0)), tf.constant([1,-1], dtype=tf.float64))\n",
    "#             z_y_new = tf.reduce_sum(tf.exp(-1*k*thetas*sbins), axis=0)*sbin_widths/(1-user_alphas)\n",
    "# *sbin_widths\n",
    "\n",
    "            Z_new = tf.reduce_sum(z_y_new)\n",
    "            print(\"new z \", Z_new)\n",
    "\n",
    "#p theta without exp [y x i]\n",
    "            # snap s_ to the nearest bin. \n",
    "            s_ = tf.round((s_-user_alphas)*numbins/(1-user_alphas))*sbin_widths + user_alphas\n",
    "            logp_theta_new_t = tf.map_fn(lambda y:  tf.reduce_sum(-y*l*pot(s_), axis=1), tf.constant([1,-1], dtype=tf.float64))\n",
    "\n",
    "\n",
    "            loss_new_t = tf.reduce_logsumexp(logp_theta_new_t, axis=0) - tf.log(Z_new)\n",
    "            loss_new = tf.negative(tf.reduce_sum(loss_new_t))\n",
    "        \n",
    "#marginals as softmax\n",
    "\n",
    "            marginals_new = tf.expand_dims(tf.nn.softmax(logp_theta_new_t, axis=0), 2)\n",
    "\n",
    "        if(penalty < 100):\n",
    "            \n",
    "            if(debug):\n",
    "                print(\"pout\",pout)    \n",
    "    #MAP\n",
    "            t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout,\\\n",
    "                               name=\"t_pout\")\n",
    "\n",
    "            if(debug):\n",
    "                print(\"t_pout\",t_pout)\n",
    "\n",
    "        t =  tf.squeeze(thetas)\n",
    "        if(debug):\n",
    "            print(\"t\",t)\n",
    "\n",
    "        if(penalty < 100):\n",
    "            marginals = tf.nn.softmax(t_pout,axis=0)\n",
    "#         else:\n",
    "#             marginals = marginals_new\n",
    "        \n",
    "            if(debug):\n",
    "                print(\"zy\",zy)\n",
    "            logz = tf.log(tf.reduce_sum(zy,axis=0),name=\"logz\")\n",
    "            if(debug):\n",
    "                print(\"logz\",logz)\n",
    "    ##         tf.summary.scalar('logz', logz)\n",
    "            lsp = tf.reduce_logsumexp(t_pout,axis=0)\n",
    "            if(debug):\n",
    "                print(\"lsp\",lsp)\n",
    "##         tf.summary.scalar('lsp', tf.reduce_sum(lsp))\n",
    "        \n",
    "    \n",
    "        if(penalty == 20 or penalty > 100):\n",
    "            a_t = tf.convert_to_tensor(a_pr, dtype=tf.float64)\n",
    "            n_t = tf.convert_to_tensor(n_pr, dtype=tf.float64)\n",
    "\n",
    "        def get_loss_20():\n",
    "#NEW MODEL\n",
    "\n",
    "            # unnormalized_marginals [numY]\n",
    "            unnormalized_marginals = [tf.reduce_prod(tf.exp(-thetas*LF_l) + 1), \\\n",
    "                                      tf.reduce_prod(tf.exp(thetas*LF_l) + 1)]\n",
    "            # same for continuous, just use the new formulation with sbins for summation, no intergration\n",
    "            Z = tf.reduce_sum(unnormalized_marginals)\n",
    "\n",
    "            # unnormalized_marginals_tiled: [numLF, numY]\n",
    "            # per-LF-marginals: [numLFs]\n",
    "            per_LF_marginals = tf.gather(unnormalized_marginals, LF_label)\n",
    "\n",
    "#             return per_LF_marginals\n",
    "            # PÎ¸(kj=y): [numLFs]  (computing Eq 14)                       \n",
    "            per_LF_agreement_prob = tf.exp(thetas)/(1 + tf.exp(thetas))*per_LF_marginals/Z\n",
    "\n",
    "#             p_thetas =  tf.expand_dims(prod__,1) * exp_terms/(1+exp_terms) / Z\n",
    "            p_thetas = per_LF_agreement_prob\n",
    "\n",
    "            ptheta_terms = a_t * n_t * tf.log(p_thetas) + (1-a_t) * n_t * tf.log(1-p_thetas)\n",
    "                                                   \n",
    "#-------------------------------\n",
    "\n",
    "            return tf.negative(tf.reduce_sum(ptheta_terms)) # add other loss component\n",
    "\n",
    "\n",
    "        if(penalty > 100):\n",
    "\n",
    "            numYs=2\n",
    "            per_lf_prob, loss_new, marginals = model_only.model(numYs, k, l, s, thetas, alphas, isdiscrete, user_a, penalty)\n",
    "            ptheta_ = model_only.precision_loss(a_t, n_t, per_lf_prob)\n",
    "            loss_precision = tf.negative(tf.reduce_sum(ptheta_))\n",
    "            \n",
    "#             xyz = sbin_widths        \n",
    "        \n",
    "        \n",
    "        \n",
    "            is_discrete = tf.convert_to_tensor(isdiscrete, dtype = tf.float64)\n",
    "        \n",
    "            user_alphas = tf.convert_to_tensor(user_a, dtype=tf.float64)\n",
    "            # TODO 1: user_alphas should be 1-eps for discrete LFs.\n",
    "            active = is_discrete + (1-is_discrete) * tf.cast(tf.greater_equal(s, user_alphas-0.0001), dtype=tf.float64)\n",
    "            s_ = tf.clip_by_value((s - user_alphas)/(1-user_alphas+0.00001), 0, 1) # tf.stop_gradient(s * active)\n",
    "            l = tf.stop_gradient(l * active)\n",
    "            \n",
    "            numbins = tf.constant(10, dtype=tf.float64)\n",
    "            #TODO: 2 binned integration over the s values to allow stable gradients.\n",
    "            sbin_widths = (1-0*user_alphas)/numbins #(1-user_alphas)/numbins\n",
    "            sbins = tf.einsum('i,j->ij', sbin_widths, tf.cast(tf.range(0,numbins+1), dtype=tf.float64))+tf.expand_dims(user_alphas,1)\n",
    "            sbins = tf.transpose(sbins)\n",
    "\n",
    "#             s_smooth = tf.round((s_-user_alphas)*numbins/(1-user_alphas))*sbin_widths + user_alphas\n",
    "#             s_ = (1-is_discrete) * s_smooth + is_discrete * s_\n",
    "            \n",
    "            ys = tf.constant([-1,1], dtype=tf.float64)\n",
    "\n",
    "#adding new\n",
    "            def cont_pot(s, y, l):\n",
    "                if(penalty%10 == 1):\n",
    "                    return (thetas*s-alphas)*y*l\n",
    "                elif(penalty%10 == 2):\n",
    "                    return y*l*thetas*tf.clip_by_value(s-alphas,0,1)\n",
    "                elif(penalty%10 == 3):\n",
    "                    return y*l*thetas*s\n",
    "                elif(penalty%10 == 4):\n",
    "                    return y*l*thetas*tf.sigmoid(s-alphas)\n",
    "                elif(penalty%10 == 5):\n",
    "                  # if labels(l,y) agree use a potential of theta*s\n",
    "                  # else if they disagree and l is non-zero use alphas*(1-s)\n",
    "                  # zero in all other cases.\n",
    "                  # the expression here might be incorrect, make sure it implements\n",
    "                  # as per the above intention.\n",
    "                    return thetas*s*tf.cast(tf.equal(y,l),dtype=tf.float64) + \\\n",
    "                            alphas*(1-s)*l*tf.cast(tf.not_equal(y,l), dtype=tf.float64)\n",
    "                elif (penalty%10 == 6):\n",
    "                  # Gaussian distribution with a variance of 1.\n",
    "                    pos = tf.cast(tf.equal(y,l),dtype=tf.float64)\n",
    "                    return -(s-thetas)*(s-thetas)*pos/2 - (1-pos)*l*(s-alphas)*(s-alphas)/2\n",
    "                return 0\n",
    "    \n",
    "            def pot(s, y, l):\n",
    "                return (1-is_discrete) * cont_pot(s, y,l) + is_discrete * y*thetas*l\n",
    "            \n",
    "            def msg(s, y, l):\n",
    "                return 1 +  (1-is_discrete) * tf.reduce_sum(tf.exp(pot(s, y,l)), axis=0)*sbin_widths +  (is_discrete) *  tf.exp(thetas*y*l)\n",
    "            \n",
    "            z_y = tf.map_fn(lambda y: tf.reduce_prod(msg(sbins, y,k)), ys)\n",
    "            Z_ = tf.reduce_sum(z_y)\n",
    "\n",
    "            log_pt = tf.map_fn(lambda y: tf.reduce_sum(pot(s_, y,l), axis=1), ys)\n",
    "            pt_1 = tf.reduce_logsumexp(log_pt, axis=1) - tf.log(Z_)\n",
    "            \n",
    "            \n",
    "            per_lf = tf.gather(z_y, LF_label)\n",
    "            prec_factor = tf.squeeze((msg(sbins, 1,1)-1) / (msg(sbins, 1,1)))\n",
    "\n",
    "            per_lf_z = tf.reduce_sum(tf.map_fn(lambda y: (msg(sbins, y,k)-1)/msg(sbins, y,k)*z_y[tf.cast((y+1)/2, dtype = tf.int32)], ys), axis=0)\n",
    "            \n",
    "#             per_lf_prob = prec_factor * per_lf / Z_\n",
    "            per_lf_prob = prec_factor * per_lf/per_lf_z\n",
    "\n",
    "            ptheta_ = a_t * n_t * tf.log(per_lf_prob) + (1-a_t) * n_t * tf.log(1-per_lf_prob)\n",
    "             \n",
    "            marginals_new = tf.expand_dims(tf.nn.softmax(log_pt, axis=0), 2)\n",
    "            marginals = marginals_new\n",
    "            \n",
    "            loss_new = tf.negative(tf.reduce_sum(pt_1))\n",
    "            loss_precision = tf.negative(tf.reduce_sum(ptheta_))\n",
    "            \n",
    "            xyz = sbin_widths\n",
    "            \n",
    "\n",
    "        prec_loss = tf.constant(0)\n",
    "    \n",
    "        if(not norm):\n",
    "            print(\"unnormlized loss\")\n",
    "            loss = tf.negative(tf.reduce_sum(lsp  ))\n",
    "        elif(penalty == 1):\n",
    "            print(\"penalty1\")\n",
    "            loss = tf.negative(tf.reduce_sum(lsp  - logz  )) \\\n",
    "                      +(g*tf.reduce_sum(tf.maximum(tf.zeros_like(thetas),-thetas)))\n",
    "        elif(penalty == 2):\n",
    "            print(\"penalty2\")\n",
    "            loss = tf.negative(tf.reduce_sum(lsp  - logz  )) \\\n",
    "                     -(g*tf.minimum( tf.reduce_min(thetas),0.0))\n",
    "            prec_loss = tf.constant(0)\n",
    "        elif(penalty == 3):\n",
    "            print(\"penalty3\")\n",
    "            loss = tf.negative(tf.reduce_sum(lsp  - logz  )) \\\n",
    "                     +(g*tf.reduce_sum(tf.log(1+tf.exp(-thetas-p3k))))\n",
    "        \n",
    "\n",
    "        elif(penalty == 4):\n",
    "            print(\"precision penalty\")\n",
    "            prec_loss = tf.reduce_sum(tf.map_fn(lambda j: softplus_p(j),np.arange(NoOfLFs),\\\n",
    "                                             dtype=tf.float64))\n",
    "            loss = tf.negative(tf.reduce_sum(lsp  - logz  )) \\\n",
    "                + (g* prec_loss)\n",
    "        \n",
    "        elif(penalty == 41):\n",
    "            print(\"precision penalty\")\n",
    "            prec_loss, xtra = sfp()\n",
    "            loss = tf.negative(tf.reduce_sum(lsp  - logz  )) \\\n",
    "                + (g * prec_loss)\n",
    "#                 + (g*tf.reduce_sum(tf.map_fn(lambda j: softplus_p(j),np.arange(NoOfLFs),\\\n",
    "#                                              dtype=tf.float64)))\n",
    "        \n",
    "        elif(penalty == 42):\n",
    "            print(\"precision penalty\")\n",
    "            temp_prloss, _ = sfpp()\n",
    "            temp_uloss = tf.negative(tf.reduce_sum(lsp  - logz  ))\n",
    "            r21 = tf.gradients(temp_uloss, thetas)\n",
    "            r22 = tf.gradients(temp_prloss, thetas)\n",
    "            xtra_tfg = tf.reduce_sum(tf.abs(r21[0][0]))/tf.reduce_sum(tf.abs(r22[0][0]))\n",
    "#             gammaR = temp_uloss/temp_prloss\n",
    "#             prec_loss, xtra = sfpp()\n",
    "            prec_loss = tf.reduce_sum(temp_prloss)\n",
    "\n",
    "            grad_ratio = 1\n",
    "\n",
    "            xtra = [xtra_tfg]\n",
    "\n",
    "            loss = temp_uloss \\\n",
    "                + (g * prec_loss)\n",
    "\n",
    "        elif(penalty == 44):\n",
    "            print(\"precision penalty\")\n",
    "            loss = tf.negative(tf.reduce_sum(lsp  - logz  )) \\\n",
    "                + (g*tf.reduce_sum(tf.map_fn(lambda j: softplus_pp(j),np.arange(NoOfLFs),\\\n",
    "                                             dtype=tf.float64)))\n",
    "                \n",
    "                \n",
    "        elif(penalty == 5):\n",
    "            print(\"precision log(softplus) penalty\")\n",
    "            loss = tf.negative(tf.reduce_sum(lsp  - logz  )) \\\n",
    "                + (g*tf.reduce_sum(tf.map_fn(lambda j: tf.log(softplus_p(j)),np.arange(NoOfLFs),\\\n",
    "                                             dtype=tf.float64)))\n",
    "        elif(penalty == 6):\n",
    "            print(\"recall penalty\")\n",
    "            loss = tf.negative(tf.reduce_sum(lsp  - logz  )) \\\n",
    "                + (g*tf.reduce_sum(tf.map_fn(lambda j: softplus_r(j),np.arange(NoOfLFs),\\\n",
    "                                             dtype=tf.float64)))\n",
    "        elif(penalty == 7):\n",
    "            print(\"precision and recall penalty\")\n",
    "            loss = tf.negative(tf.reduce_sum(lsp  - logz  )) \\\n",
    "                + (g*tf.reduce_sum(tf.map_fn(lambda j: softplus_p(j),np.arange(NoOfLFs),\\\n",
    "                                             dtype=tf.float64))) \\\n",
    "                + (g*tf.reduce_sum(tf.map_fn(lambda j: softplus_r(j),np.arange(NoOfLFs),\\\n",
    "                                             dtype=tf.float64)))\n",
    "        elif(penalty == 8):\n",
    "            print(\"precision and sign 1 penalty\")\n",
    "            loss = tf.negative(tf.reduce_sum(lsp  - logz  )) \\\n",
    "                + (g*tf.reduce_sum(tf.map_fn(lambda j: softplus_p(j),np.arange(NoOfLFs),\\\n",
    "                                             dtype=tf.float64))) \\\n",
    "                + (g*tf.reduce_sum(tf.maximum(tf.zeros_like(thetas),-thetas)))\n",
    "        elif(penalty == 9):\n",
    "            print(\"precision and sign 2 penalty\")\n",
    "            loss = tf.negative(tf.reduce_sum(lsp  - logz  )) \\\n",
    "                + (g*tf.reduce_sum(tf.map_fn(lambda j: softplus_p(j),np.arange(NoOfLFs),\\\n",
    "                                             dtype=tf.float64))) \\\n",
    "                - (g*tf.minimum( tf.reduce_min(thetas),0.0))\n",
    "        elif(penalty == 10):\n",
    "            print(\"precision and sign 3 penalty\")\n",
    "            loss = tf.negative(tf.reduce_sum(lsp  - logz  )) \\\n",
    "                + (g*tf.reduce_sum(tf.map_fn(lambda j: softplus_p(j),np.arange(NoOfLFs),\\\n",
    "                                             dtype=tf.float64))) \\\n",
    "                + (g*tf.reduce_sum(tf.log(1+tf.exp(-thetas-p3k))))\n",
    "        elif(penalty == 11):\n",
    "            print(\"precision and recall and sign 2 penalty\")\n",
    "            loss = tf.negative(tf.reduce_sum(lsp  - logz  )) \\\n",
    "                + (g*tf.reduce_sum(tf.map_fn(lambda j: softplus_p(j),np.arange(NoOfLFs),\\\n",
    "                                             dtype=tf.float64))) \\\n",
    "                + (g*tf.reduce_sum(tf.map_fn(lambda j: softplus_r(j),np.arange(NoOfLFs),\\\n",
    "                                             dtype=tf.float64))) \\\n",
    "                  - (g*tf.minimum( tf.reduce_min(thetas),0.0))\n",
    "        elif(penalty == 12):\n",
    "            print(\"recall and sign 2 penalty\")\n",
    "            loss = tf.negative(tf.reduce_sum(lsp  - logz  )) \\\n",
    "                + (g*tf.reduce_sum(tf.map_fn(lambda j: softplus_r(j),np.arange(NoOfLFs),\\\n",
    "                                             dtype=tf.float64))) \\\n",
    "                  - (g*tf.minimum( tf.reduce_min(thetas),0.0))\n",
    "        elif(penalty == 15):\n",
    "            print(\"equation 15 and sign 2 penalty\")\n",
    "            loss = tf.negative(tf.reduce_sum(lsp  - logz  )) \\\n",
    "                + (g*tf.reduce_sum(tf.map_fn(lambda j: softplus_r15(j),np.arange(NoOfLFs),\\\n",
    "                                             dtype=tf.float64))) \\\n",
    "                  - (g*tf.minimum( tf.reduce_min(thetas),0.0))\n",
    "\n",
    "\n",
    "\n",
    "                    \n",
    "        elif(penalty == 20):\n",
    "            print(\"new model\")\n",
    "            prec_loss = get_loss_20()\n",
    "            loss =  tf.negative(tf.reduce_sum(lsp  - logz  )) + prec_loss\n",
    "                  \n",
    "        elif(penalty >100 and penalty<111):\n",
    "            sys.stdout = stdout_saved\n",
    "            print(\"new model smooth \", penalty)\n",
    "            sys.stdout = open(\"trash\", \"w\")            \n",
    "            print(\"new model smooth \", penalty)\n",
    "            loss = loss_new\n",
    "#             loss, _ = smooth_new_prec()\n",
    "            \n",
    "                    \n",
    "        elif(penalty >110):\n",
    "            sys.stdout = stdout_saved\n",
    "            print(\"new model smooth with constraints \", penalty)\n",
    "            sys.stdout = open(\"trash\", \"w\")            \n",
    "            print(\"new model smooth with constraints \", penalty)\n",
    "            loss = loss_new + loss_precision\n",
    "#             loss_un, loss_pr = smooth_new_prec()\n",
    "#             loss = loss_un + loss_pr\n",
    "\n",
    "        else:\n",
    "            print(\"normalized loss\")\n",
    "            loss = tf.negative(tf.reduce_sum(lsp  - logz  ))\n",
    "            prec_loss = tf.constant(0)\n",
    "       \n",
    "        if(debug):\n",
    "            print(\"loss\",loss)\n",
    "\n",
    "#         marginals = tf.nn.softmax(t_pout,axis=0)\n",
    "\n",
    "        if(debug):\n",
    "            print(\"marginals\",marginals)\n",
    "        predict = tf.argmax(marginals,axis=0)\n",
    "\n",
    "\n",
    "        train_step = tf.train.AdamOptimizer(lr).minimize(loss)#, var_list=[thetas, alphas]) \n",
    "\n",
    "\n",
    "        init_g = tf.global_variables_initializer()\n",
    "        init_l = tf.local_variables_initializer()\n",
    "        \n",
    "        dev_high = np.zeros(3)\n",
    "        test_high = np.zeros(3)\n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init_g)\n",
    "            sess.run(init_l)\n",
    "\n",
    "            flg = False\n",
    "            # Initialize an iterator over the training dataset.\n",
    "            for en in range(ep):\n",
    "                sess.run(train_init_op)\n",
    "#                 temprun = sess.run(next_element2)\n",
    "#                 print('len next ',len(temprun), len(temprun[0]))\n",
    "#                 print(temprun[0])\n",
    "                tl = 0\n",
    "                pll = 0\n",
    "                unl = 0\n",
    "                try:\n",
    "                    it = 0\n",
    "                    \n",
    "                    grad_sum = 0.0\n",
    "                    while True:\n",
    "#                         print(en, it)\n",
    "\n",
    "                        _,ls,ploss,xpp,t = sess.run([train_step,loss,prec_loss,xtra,thetas])\n",
    "                        if(penalty == 42):\n",
    "                            un_loss = sess.run(temp_uloss)\n",
    "\n",
    "                        tl = tl + ls\n",
    "                        pll = pll + ploss\n",
    "                        if(penalty == 42):\n",
    "                            unl = unl + un_loss\n",
    "                        it = it + 1\n",
    "#                         newptheta = sess.run(sbins)\n",
    "#                         ztemp = sess.run(xyz)\n",
    "                \n",
    "#                         print(it*BATCH_SIZE)\n",
    "                \n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    pass\n",
    "                opstring = \"\"\n",
    "                print(en,\"loss\",tl)\n",
    "                opstring = opstring + str(en) + \",\" + str(tl) + \",\"\n",
    "#                 print('temp__', temp__)\n",
    "                print(\"un loss\", unl)\n",
    "                print(\"ploss\", pll)\n",
    "                print(\"ratio\", unl/pll)\n",
    "                print(\"extra\", xpp)\n",
    "#                 dloss = sess.run(dev_loss)\n",
    "#                 print(\"dloss\", dloss)\n",
    "#                 print(\"expected ratio\", xpp/(xpp+1))\n",
    "                print(\"dev set\")\n",
    "                sess.run(dev_init_op)\n",
    "##                 sm,a,t,m,pl = sess.run([summary_merged,alphas,thetas,marginals,predict])\n",
    "#                 a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "                a,t,m,pl,ddloss = sess.run([alphas,thetas,marginals,predict,loss])\n",
    "##                 test_writer.add_summary(sm, en)\n",
    "                print('init dev loss', ddloss)\n",
    "                opstring = opstring + str(ddloss) + \",\"\n",
    "                print(a)\n",
    "                print(t)\n",
    "                unique, counts = np.unique(pl, return_counts=True)\n",
    "                print(dict(zip(unique, counts)))\n",
    "                print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "                dev_results = precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\")\n",
    "                print('devresults ', dev_results)\n",
    "                roc_dev = roc_auc_score(np.array(gold_labels_dev), np.array(m[1,:,0]))\n",
    "                print('roc score', roc_dev)\n",
    "                opstring = str(en) + ',' #do not print losses\n",
    "                opstring = opstring + str(dev_results[2]) + \",\" + str(roc_dev) + \",\"\n",
    "#                 if(dev_results[2]>dev_high[2]):\n",
    "                dev_high = np.array(dev_results)\n",
    "#                 dev_high = max(dev_high, dev_results[2])\n",
    "                print()\n",
    "                print(\"test set\")\n",
    "                sess.run(test_init_op)\n",
    "                a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "                unique, counts = np.unique(pl, return_counts=True)\n",
    "                print(dict(zip(unique, counts)))\n",
    "                print(\"acc\",accuracy_score(gold_labels_test,pl))\n",
    "                test_results = precision_recall_fscore_support(np.array(gold_labels_test),np.array(pl),average=\"binary\")\n",
    "                print('testresults ', test_results)\n",
    "                roc_test = roc_auc_score(np.array(gold_labels_test), np.array(m[1,:,0]))\n",
    "                print('roc score', roc_test)\n",
    "                opstring = opstring + str(test_results[2]) + \",\" + str(roc_test)\n",
    "#                 if(test_results[2]>test_high[2]):\n",
    "                test_high = np.array(test_results)\n",
    "                print()\n",
    "        \n",
    "                sys.stdout = stdout_saved\n",
    "                print(opstring)\n",
    "# #                 newptheta = sess.run(marginals)\n",
    "#                 print(\"new z \", ztemp)\n",
    "#                 print(\"new z shape \", np.array(ztemp).shape)\n",
    "#                 print('shape ',np.array(newptheta).shape)\n",
    "                sys.stdout = open(\"trash\", \"w\")\n",
    "                \n",
    "\n",
    "    return pl, m, dev_high, opstring#test_high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
